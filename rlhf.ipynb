{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# InstructGPT\n",
        "\n",
        "## Overview\n",
        "\n",
        "The history of language models can be understood through one key constraint: **alignment with human intent**.\n",
        "\n",
        "GPT-3 was powerful but impractical. It needed complex prompts to produce useful outputs. Most users couldn't effectively use it.\n",
        "\n",
        "InstructGPT solved this with a simple insight: use human feedback to teach the model what we actually want.\n",
        "\n",
        "This notebook implements a simplified version of InstructGPT to understand how RLHF works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Problem\n",
        "\n",
        "Language models learn to predict the next word. This doesn't naturally align with being helpful.\n",
        "\n",
        "GPT-3 would often:\n",
        "- Ignore instructions\n",
        "- Generate plausible but false information\n",
        "- Produce unhelpful responses\n",
        "\n",
        "The gap between \"predicting text\" and \"being helpful\" was the core problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Solution: RLHF\n",
        "\n",
        "InstructGPT introduced a 3-step process:\n",
        "\n",
        "**1. Supervised Fine-Tuning (SFT)**\n",
        "- Collect examples of good responses\n",
        "- Fine-tune the model on these examples\n",
        "\n",
        "**2. Reward Model**  \n",
        "- Have humans compare different responses\n",
        "- Train a model to predict human preferences\n",
        "\n",
        "**3. Reinforcement Learning**\n",
        "- Use the reward model to optimize responses\n",
        "- Balance between being helpful and staying coherent\n",
        "\n",
        "Let's implement each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "\n",
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_name = \"gpt2\"  # Using small GPT-2 for speed\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Supervised Fine-Tuning\n",
        "\n",
        "First, we teach the model to follow instructions using examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base GPT-2\n",
        "base_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Create training examples\n",
        "# In real InstructGPT, humans wrote thousands of these\n",
        "examples = [\n",
        "    {\n",
        "        \"instruction\": \"Explain quantum computing in one sentence\",\n",
        "        \"response\": \"Quantum computing uses quantum physics to process information in ways classical computers cannot.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"What is 2+2?\",\n",
        "        \"response\": \"2+2 equals 4.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Write a haiku about coding\",\n",
        "        \"response\": \"Bugs hide in the code\\nPatience reveals the answer\\nSuccess compiles slow\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Format for training\n",
        "def format_example(ex):\n",
        "    return {\"text\": f\"Human: {ex['instruction']}\\nAssistant: {ex['response']}\"}\n",
        "\n",
        "dataset = Dataset.from_list(examples).map(format_example)\n",
        "print(f\"Training on {len(dataset)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple training loop\n",
        "def train_sft(model, dataset, epochs=5):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for example in dataset:\n",
        "            # Tokenize\n",
        "            inputs = tokenizer(example['text'], return_tensors='pt', \n",
        "                             max_length=128, truncation=True, padding=True)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Train\n",
        "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(dataset):.4f}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train SFT model\n",
        "sft_model = train_sft(base_model, dataset)\n",
        "print(\"\\nSFT training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the SFT model\n",
        "def generate(model, instruction):\n",
        "    prompt = f\"Human: {instruction}\\nAssistant:\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=100, \n",
        "                               temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"Assistant:\")[-1].strip()\n",
        "\n",
        "# Compare base vs SFT\n",
        "test_instruction = \"What is machine learning?\"\n",
        "\n",
        "print(f\"Instruction: {test_instruction}\")\n",
        "print(f\"\\nBase GPT-2: {generate(base_model, test_instruction)}\")\n",
        "print(f\"\\nSFT Model: {generate(sft_model, test_instruction)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Reward Model\n",
        "\n",
        "Now we need to teach a model to recognize good responses. In real InstructGPT, humans compared thousands of response pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple reward model\n",
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base = base_model\n",
        "        self.reward_head = nn.Linear(768, 1)  # GPT-2 hidden size = 768\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base.transformer(input_ids, attention_mask=attention_mask)\n",
        "        # Use last token's hidden state\n",
        "        last_hidden = outputs.last_hidden_state[:, -1, :]\n",
        "        reward = self.reward_head(last_hidden)\n",
        "        return reward\n",
        "\n",
        "# Create reward model\n",
        "reward_model = RewardModel(GPT2LMHeadModel.from_pretrained(model_name)).to(device)\n",
        "\n",
        "# Simulate preference data\n",
        "# In reality, humans would rank these\n",
        "def score_response(response):\n",
        "    \"\"\"Simple heuristic to simulate human preferences\"\"\"\n",
        "    score = 0\n",
        "    if len(response) > 10 and len(response) < 100:  # Good length\n",
        "        score += 1\n",
        "    if response.endswith('.'):  # Complete sentence\n",
        "        score += 1\n",
        "    if len(set(response.split())) / len(response.split()) > 0.7:  # Not repetitive\n",
        "        score += 1\n",
        "    return score\n",
        "\n",
        "print(\"Reward model created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Reinforcement Learning\n",
        "\n",
        "Finally, we optimize the model using the reward signal. This is a simplified version of PPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple RL training (very simplified PPO)\n",
        "def train_with_rewards(model, reward_model, instructions, steps=10):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "    \n",
        "    for step in range(steps):\n",
        "        instruction = np.random.choice(instructions)\n",
        "        \n",
        "        # Generate response\n",
        "        response = generate(model, instruction)\n",
        "        \n",
        "        # Get reward\n",
        "        reward = score_response(response)\n",
        "        \n",
        "        # Update model (simplified - real PPO is more complex)\n",
        "        prompt = f\"Human: {instruction}\\nAssistant: {response}\"\n",
        "        inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=128)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "        loss = -reward * outputs.loss  # Maximize reward\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if step % 2 == 0:\n",
        "            print(f\"Step {step}: Reward = {reward}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train with RL\n",
        "rl_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "rl_model.load_state_dict(sft_model.state_dict())  # Start from SFT\n",
        "\n",
        "training_instructions = [\n",
        "    \"Explain gravity\",\n",
        "    \"What is Python?\",\n",
        "    \"How do computers work?\"\n",
        "]\n",
        "\n",
        "rl_model = train_with_rewards(rl_model, reward_model, training_instructions)\n",
        "print(\"\\nRL training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all three models\n",
        "test_instruction = \"How does the internet work?\"\n",
        "\n",
        "print(f\"Instruction: {test_instruction}\\n\")\n",
        "\n",
        "# Reload base model for fair comparison\n",
        "base_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "print(\"Base GPT-2:\")\n",
        "print(generate(base_model, test_instruction))\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "print(\"After SFT:\")\n",
        "print(generate(sft_model, test_instruction))\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "print(\"After RLHF:\")\n",
        "print(generate(rl_model, test_instruction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Insights\n",
        "\n",
        "**What made InstructGPT successful:**\n",
        "\n",
        "1. **Quality over quantity**: A small amount of high-quality human feedback was more valuable than massive datasets\n",
        "\n",
        "2. **Alignment matters**: Teaching models what we want is as important as making them more capable\n",
        "\n",
        "3. **Iterative improvement**: Each step (SFT \u2192 Reward Model \u2192 RL) builds on the previous\n",
        "\n",
        "**The impact:**\n",
        "\n",
        "- InstructGPT (1.3B parameters) was preferred over GPT-3 (175B parameters)\n",
        "- This approach enabled ChatGPT and the current AI revolution\n",
        "- RLHF remains the core technique for aligning modern LLMs\n",
        "\n",
        "The constraint wasn't compute or model size - it was alignment with human intent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "We implemented a simplified InstructGPT:\n",
        "\n",
        "\u2713 **SFT**: Taught the model to follow instructions  \n",
        "\u2713 **Reward Model**: Learned to recognize good responses  \n",
        "\u2713 **RL**: Optimized for human preferences  \n",
        "\n",
        "This simple process transformed an unpredictable text generator into a helpful assistant.\n",
        "\n",
        "The real breakthrough wasn't making models bigger - it was making them do what we actually want."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}