{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRPO (Group Relative Policy Optimization) for Code Generation\n",
        "\n",
        "## From Math Models to Code Masters\n",
        "\n",
        "This notebook implements GRPO to transform a base model into a proficient code generator. GRPO was the key technique behind DeepSeek-R1's remarkable reasoning capabilities.\n",
        "\n",
        "### What is GRPO?\n",
        "\n",
        "**Group Relative Policy Optimization (GRPO)** is a reinforcement learning algorithm that improves upon PPO by:\n",
        "1. **Eliminating the value function** - saving memory by not needing a separate critic network\n",
        "2. **Using group-based advantages** - generating multiple responses per prompt and comparing them\n",
        "3. **Relative scoring** - using the mean reward of the group as baseline\n",
        "\n",
        "This makes GRPO particularly suitable for training large language models on limited hardware (like T4 GPUs on Colab)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The GRPO Algorithm\n",
        "\n",
        "GRPO works by:\n",
        "1. **Generate multiple responses** for each prompt (e.g., 4-8 solutions per coding problem)\n",
        "2. **Score each response** using a reward model (correctness checker for code)\n",
        "3. **Calculate advantages** by comparing each response to the group mean\n",
        "4. **Update the policy** to favor high-advantage responses\n",
        "\n",
        "Mathematical formulation:\n",
        "- For prompt $s_j$, generate $K$ responses: $a_{j1}, a_{j2}, ..., a_{jK}$\n",
        "- Each response gets reward $R_{jk}$\n",
        "- Group mean: $\\bar{R}_j = \\frac{1}{K} \\sum_{k=1}^{K} R_{jk}$\n",
        "- Advantage: $A_{jk} = R_{jk} - \\bar{R}_j$\n",
        "\n",
        "Policy update objective:\n",
        "$$\\mathcal{L} = -\\sum_{j,k} \\min\\left(\\frac{\\pi_\\theta(a_{jk}|s_j)}{\\pi_{\\theta_{old}}(a_{jk}|s_j)} A_{jk}, \\text{clip}\\left(\\frac{\\pi_\\theta(a_{jk}|s_j)}{\\pi_{\\theta_{old}}(a_{jk}|s_j)}, 1-\\epsilon, 1+\\epsilon\\right) A_{jk}\\right) + \\beta \\cdot KL(\\pi_\\theta || \\pi_{ref})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation Plan\n",
        "\n",
        "1. **Setup**: Load base model and prepare code generation datasets\n",
        "2. **Reward Model**: Implement code correctness checker (execution-based rewards)\n",
        "3. **GRPO Core**: Build the group-based advantage estimation\n",
        "4. **Training Loop**: Implement the GRPO training algorithm\n",
        "5. **Evaluation**: Test on code generation benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Setup and Installation\n",
        "\n",
        "First, let's install the required packages. We'll use smaller models that can run on T4 GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Google Colab\n",
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q transformers datasets accelerate bitsandbytes\n",
        "!pip install -q torch trl peft\n",
        "!pip install -q sentencepiece protobuf\n",
        "!pip install -q wandb  # for tracking experiments (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, \n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import json\n",
        "import subprocess\n",
        "import tempfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Load the Base Model\n",
        "\n",
        "We'll use a smaller model that fits on T4 GPU. Since Qwen2.5-Math models might be large, we'll use quantization or a smaller variant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"  # Starting with a smaller model for T4 compatibility\n",
        "# Alternative options:\n",
        "# model_name = \"microsoft/phi-2\"  # 2.7B params\n",
        "# model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "\n",
        "# Load model with 4-bit quantization for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"left\"  # Important for batch generation\n",
        ")\n",
        "\n",
        "# Set pad token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded! Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Prepare Code Generation Dataset\n",
        "\n",
        "We'll create a dataset of coding problems with test cases for automatic evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple code generation dataset\n",
        "# In practice, you'd use datasets like HumanEval, MBPP, or custom datasets\n",
        "\n",
        "coding_problems = [\n",
        "    {\n",
        "        \"prompt\": \"Write a Python function to find the sum of two numbers.\",\n",
        "        \"function_name\": \"add_numbers\",\n",
        "        \"test_cases\": [\n",
        "            {\"inputs\": [2, 3], \"expected\": 5},\n",
        "            {\"inputs\": [10, -5], \"expected\": 5},\n",
        "            {\"inputs\": [0, 0], \"expected\": 0}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Write a Python function to check if a number is prime.\",\n",
        "        \"function_name\": \"is_prime\",\n",
        "        \"test_cases\": [\n",
        "            {\"inputs\": [2], \"expected\": True},\n",
        "            {\"inputs\": [4], \"expected\": False},\n",
        "            {\"inputs\": [17], \"expected\": True},\n",
        "            {\"inputs\": [1], \"expected\": False}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Write a Python function to reverse a string.\",\n",
        "        \"function_name\": \"reverse_string\",\n",
        "        \"test_cases\": [\n",
        "            {\"inputs\": [\"hello\"], \"expected\": \"olleh\"},\n",
        "            {\"inputs\": [\"Python\"], \"expected\": \"nohtyP\"},\n",
        "            {\"inputs\": [\"\"], \"expected\": \"\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Write a Python function to find the factorial of a number.\",\n",
        "        \"function_name\": \"factorial\",\n",
        "        \"test_cases\": [\n",
        "            {\"inputs\": [0], \"expected\": 1},\n",
        "            {\"inputs\": [5], \"expected\": 120},\n",
        "            {\"inputs\": [3], \"expected\": 6}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Write a Python function to find the largest element in a list.\",\n",
        "        \"function_name\": \"find_max\",\n",
        "        \"test_cases\": [\n",
        "            {\"inputs\": [[1, 3, 2]], \"expected\": 3},\n",
        "            {\"inputs\": [[-1, -5, -2]], \"expected\": -1},\n",
        "            {\"inputs\": [[42]], \"expected\": 42}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Format prompts for the model\n",
        "def format_prompt(problem):\n",
        "    return f\"\"\"### Task: {problem['prompt']}\n",
        "\n",
        "Please write a Python function named `{problem['function_name']}` that solves this task.\n",
        "\n",
        "### Solution:\n",
        "```python\n",
        "def {problem['function_name']}(\"\"\"\n",
        "\n",
        "print(f\"Dataset size: {len(coding_problems)} problems\")\n",
        "print(\"\\nExample prompt:\")\n",
        "print(format_prompt(coding_problems[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Implement the Reward Model\n",
        "\n",
        "The reward model evaluates code by running test cases. This is a key component of GRPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_code(text: str) -> str:\n",
        "    \"\"\"Extract Python code from model output.\"\"\"\n",
        "    # Look for code between ```python and ```\n",
        "    if \"```python\" in text:\n",
        "        start = text.find(\"```python\") + 9\n",
        "        end = text.find(\"```\", start)\n",
        "        if end != -1:\n",
        "            return text[start:end].strip()\n",
        "    \n",
        "    # If no markdown, try to find function definition\n",
        "    lines = text.split('\\n')\n",
        "    code_lines = []\n",
        "    in_function = False\n",
        "    \n",
        "    for line in lines:\n",
        "        if line.strip().startswith('def '):\n",
        "            in_function = True\n",
        "        if in_function:\n",
        "            code_lines.append(line)\n",
        "            # Simple heuristic: stop at empty line after function\n",
        "            if line.strip() == '' and len(code_lines) > 1:\n",
        "                break\n",
        "    \n",
        "    return '\\n'.join(code_lines).strip()\n",
        "\n",
        "def execute_code_with_tests(code: str, test_cases: List[Dict], function_name: str) -> Tuple[float, str]:\n",
        "    \"\"\"\n",
        "    Execute code and run test cases.\n",
        "    Returns: (reward, feedback)\n",
        "    \"\"\"\n",
        "    if not code:\n",
        "        return 0.0, \"No code found\"\n",
        "    \n",
        "    try:\n",
        "        # Create a temporary file to execute code safely\n",
        "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
        "            # Write the code\n",
        "            f.write(code)\n",
        "            f.write(\"\\n\\n\")\n",
        "            \n",
        "            # Write test execution\n",
        "            f.write(\"import json\\n\")\n",
        "            f.write(f\"test_results = []\\n\")\n",
        "            \n",
        "            for i, test in enumerate(test_cases):\n",
        "                f.write(f\"try:\\n\")\n",
        "                f.write(f\"    result = {function_name}(*{json.dumps(test['inputs'])})\\n\")\n",
        "                f.write(f\"    test_results.append({{\\n\")\n",
        "                f.write(f\"        'passed': result == {json.dumps(test['expected'])},\\n\")\n",
        "                f.write(f\"        'expected': {json.dumps(test['expected'])},\\n\")\n",
        "                f.write(f\"        'got': result\\n\")\n",
        "                f.write(f\"    }})\\n\")\n",
        "                f.write(f\"except Exception as e:\\n\")\n",
        "                f.write(f\"    test_results.append({{\\n\")\n",
        "                f.write(f\"        'passed': False,\\n\")\n",
        "                f.write(f\"        'error': str(e)\\n\")\n",
        "                f.write(f\"    }})\\n\")\n",
        "            \n",
        "            f.write(\"\\nprint(json.dumps(test_results))\\n\")\n",
        "            temp_file = f.name\n",
        "        \n",
        "        # Execute the code with timeout\n",
        "        result = subprocess.run(\n",
        "            ['python3', temp_file],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=5\n",
        "        )\n",
        "        \n",
        "        # Clean up\n",
        "        os.unlink(temp_file)\n",
        "        \n",
        "        if result.returncode != 0:\n",
        "            return 0.0, f\"Code execution failed: {result.stderr}\"\n",
        "        \n",
        "        # Parse test results\n",
        "        try:\n",
        "            test_results = json.loads(result.stdout.strip())\n",
        "            passed = sum(1 for t in test_results if t.get('passed', False))\n",
        "            total = len(test_results)\n",
        "            \n",
        "            reward = passed / total\n",
        "            feedback = f\"Passed {passed}/{total} tests\"\n",
        "            \n",
        "            if reward < 1.0:\n",
        "                failed_tests = [i for i, t in enumerate(test_results) if not t.get('passed', False)]\n",
        "                feedback += f\" (failed: {failed_tests})\"\n",
        "            \n",
        "            return reward, feedback\n",
        "            \n",
        "        except json.JSONDecodeError:\n",
        "            return 0.0, \"Failed to parse test results\"\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        return 0.0, \"Code execution timeout\"\n",
        "    except Exception as e:\n",
        "        return 0.0, f\"Error: {str(e)}\"\n",
        "\n",
        "# Test the reward model\n",
        "test_code = \"\"\"def add_numbers(a, b):\n",
        "    return a + b\"\"\"\n",
        "\n",
        "reward, feedback = execute_code_with_tests(\n",
        "    test_code, \n",
        "    coding_problems[0]['test_cases'],\n",
        "    coding_problems[0]['function_name']\n",
        ")\n",
        "print(f\"Test reward: {reward}, Feedback: {feedback}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. GRPO Core Implementation\n",
        "\n",
        "Now we implement the core GRPO algorithm. The key insight is using group-based advantages without a value function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRPOTrainer:\n",
        "    \"\"\"\n",
        "    Group Relative Policy Optimization Trainer\n",
        "    Based on the DeepSeekMath paper implementation\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        learning_rate: float = 1e-6,\n",
        "        kl_coef: float = 0.05,\n",
        "        clip_range: float = 0.2,\n",
        "        group_size: int = 4,  # Number of responses per prompt\n",
        "        max_length: int = 512,\n",
        "        temperature: float = 0.7,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        \n",
        "        # GRPO hyperparameters\n",
        "        self.kl_coef = kl_coef\n",
        "        self.clip_range = clip_range\n",
        "        self.group_size = group_size\n",
        "        self.max_length = max_length\n",
        "        self.temperature = temperature\n",
        "        \n",
        "        # Reference model for KL divergence (frozen copy)\n",
        "        self.ref_model = None\n",
        "        \n",
        "    def create_reference_model(self):\n",
        "        \"\"\"Create a frozen copy of the model for KL divergence calculation\"\"\"\n",
        "        # Note: In practice with limited memory, you might load this on demand\n",
        "        # or use the same model in eval mode\n",
        "        print(\"Creating reference model...\")\n",
        "        self.ref_model = self.model  # Simplified: using same model\n",
        "        \n",
        "    def generate_responses(self, prompts: List[str]) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Generate multiple responses for each prompt\n",
        "        Returns: List of lists, where each inner list contains group_size responses\n",
        "        \"\"\"\n",
        "        all_responses = []\n",
        "        \n",
        "        for prompt in prompts:\n",
        "            responses = []\n",
        "            \n",
        "            # Generate group_size responses for this prompt\n",
        "            for _ in range(self.group_size):\n",
        "                inputs = self.tokenizer(\n",
        "                    prompt,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=self.max_length\n",
        "                ).to(self.model.device)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=self.max_length,\n",
        "                        temperature=self.temperature,\n",
        "                        do_sample=True,\n",
        "                        pad_token_id=self.tokenizer.pad_token_id,\n",
        "                        eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    )\n",
        "                \n",
        "                # Decode only the generated part\n",
        "                generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "                responses.append(response)\n",
        "            \n",
        "            all_responses.append(responses)\n",
        "        \n",
        "        return all_responses\n",
        "    \n",
        "    def compute_rewards(self, responses: List[List[str]], problems: List[Dict]) -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Compute rewards for each response using the reward model\n",
        "        \"\"\"\n",
        "        all_rewards = []\n",
        "        \n",
        "        for response_group, problem in zip(responses, problems):\n",
        "            rewards = []\n",
        "            \n",
        "            for response in response_group:\n",
        "                # Extract code and compute reward\n",
        "                code = extract_code(response)\n",
        "                reward, feedback = execute_code_with_tests(\n",
        "                    code,\n",
        "                    problem['test_cases'],\n",
        "                    problem['function_name']\n",
        "                )\n",
        "                rewards.append(reward)\n",
        "            \n",
        "            all_rewards.append(rewards)\n",
        "        \n",
        "        return all_rewards\n",
        "    \n",
        "    def compute_advantages(self, rewards: List[List[float]]) -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Compute group-relative advantages (key GRPO innovation)\n",
        "        A_jk = (R_jk - mean(R_j)) / (std(R_j) + eps)\n",
        "        \"\"\"\n",
        "        advantages = []\n",
        "        \n",
        "        for reward_group in rewards:\n",
        "            rewards_tensor = torch.tensor(reward_group, dtype=torch.float32)\n",
        "            \n",
        "            # Compute mean and std for the group\n",
        "            mean_reward = rewards_tensor.mean()\n",
        "            std_reward = rewards_tensor.std() + 1e-8\n",
        "            \n",
        "            # Normalize to get advantages\n",
        "            group_advantages = ((rewards_tensor - mean_reward) / std_reward).tolist()\n",
        "            advantages.append(group_advantages)\n",
        "        \n",
        "        return advantages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def compute_policy_loss(self, prompts, responses, advantages):\n",
        "        \"\"\"\n",
        "        Compute the GRPO policy loss with clipping and KL penalty\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        num_samples = 0\n",
        "        \n",
        "        for prompt, response_group, advantage_group in zip(prompts, responses, advantages):\n",
        "            for response, advantage in zip(response_group, advantage_group):\n",
        "                # Tokenize prompt and response\n",
        "                full_text = prompt + response\n",
        "                inputs = self.tokenizer(\n",
        "                    full_text,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=self.max_length\n",
        "                ).to(self.model.device)\n",
        "                \n",
        "                # Get model outputs\n",
        "                outputs = self.model(**inputs, labels=inputs['input_ids'])\n",
        "                \n",
        "                # Compute log probabilities\n",
        "                logprobs = -outputs.loss  # Simplified: using loss as proxy\n",
        "                \n",
        "                # For proper implementation, compute token-level log probs\n",
        "                # and importance sampling ratio\n",
        "                \n",
        "                # Clipped objective (simplified)\n",
        "                ratio = torch.exp(logprobs)  # Simplified ratio\n",
        "                clipped_ratio = torch.clamp(ratio, 1 - self.clip_range, 1 + self.clip_range)\n",
        "                \n",
        "                # Policy loss\n",
        "                policy_loss = -torch.min(\n",
        "                    ratio * advantage,\n",
        "                    clipped_ratio * advantage\n",
        "                )\n",
        "                \n",
        "                # KL penalty (simplified)\n",
        "                kl_penalty = self.kl_coef * torch.abs(logprobs)\n",
        "                \n",
        "                # Total loss\n",
        "                loss = policy_loss + kl_penalty\n",
        "                total_loss += loss\n",
        "                num_samples += 1\n",
        "        \n",
        "        return total_loss / num_samples\n",
        "    \n",
        "    def train_step(self, problems: List[Dict]):\n",
        "        \"\"\"\n",
        "        One training step of GRPO\n",
        "        \"\"\"\n",
        "        # Format prompts\n",
        "        prompts = [format_prompt(p) for p in problems]\n",
        "        \n",
        "        # 1. Generate multiple responses per prompt\n",
        "        print(f\"Generating {self.group_size} responses per prompt...\")\n",
        "        responses = self.generate_responses(prompts)\n",
        "        \n",
        "        # 2. Compute rewards\n",
        "        print(\"Computing rewards...\")\n",
        "        rewards = self.compute_rewards(responses, problems)\n",
        "        \n",
        "        # 3. Compute advantages\n",
        "        print(\"Computing advantages...\")\n",
        "        advantages = self.compute_advantages(rewards)\n",
        "        \n",
        "        # 4. Update policy\n",
        "        print(\"Updating policy...\")\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.compute_policy_loss(prompts, responses, advantages)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Log statistics\n",
        "        avg_reward = sum(sum(r) for r in rewards) / sum(len(r) for r in rewards)\n",
        "        print(f\"Average reward: {avg_reward:.3f}\")\n",
        "        print(f\"Loss: {loss.item():.3f}\")\n",
        "        \n",
        "        return {\n",
        "            'loss': loss.item(),\n",
        "            'avg_reward': avg_reward,\n",
        "            'rewards': rewards,\n",
        "            'advantages': advantages\n",
        "        }\n",
        "\n",
        "# Initialize GRPO trainer\n",
        "print(\"\\nInitializing GRPO trainer...\")\n",
        "grpo_trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    learning_rate=1e-6,\n",
        "    kl_coef=0.05,\n",
        "    clip_range=0.2,\n",
        "    group_size=4,  # Generate 4 responses per prompt\n",
        "    max_length=512,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"GRPO trainer initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Training Loop\n",
        "\n",
        "Now let's train the model using GRPO. We'll use a small number of iterations for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "num_epochs = 3\n",
        "batch_size = 2  # Number of problems per batch\n",
        "\n",
        "# Training history\n",
        "training_history = {\n",
        "    'loss': [],\n",
        "    'avg_reward': [],\n",
        "    'epoch_rewards': []\n",
        "}\n",
        "\n",
        "print(\"Starting GRPO training...\")\n",
        "print(f\"Training for {num_epochs} epochs with batch size {batch_size}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    epoch_losses = []\n",
        "    epoch_rewards = []\n",
        "    \n",
        "    # Train on batches\n",
        "    for i in range(0, len(coding_problems), batch_size):\n",
        "        batch_problems = coding_problems[i:i + batch_size]\n",
        "        \n",
        "        print(f\"\\nBatch {i//batch_size + 1}/{len(coding_problems)//batch_size}\")\n",
        "        \n",
        "        # Perform training step\n",
        "        metrics = grpo_trainer.train_step(batch_problems)\n",
        "        \n",
        "        epoch_losses.append(metrics['loss'])\n",
        "        epoch_rewards.append(metrics['avg_reward'])\n",
        "    \n",
        "    # Log epoch statistics\n",
        "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    avg_reward = sum(epoch_rewards) / len(epoch_rewards)\n",
        "    \n",
        "    training_history['loss'].append(avg_loss)\n",
        "    training_history['avg_reward'].append(avg_reward)\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
        "    print(f\"  Average Loss: {avg_loss:.3f}\")\n",
        "    print(f\"  Average Reward: {avg_reward:.3f}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Evaluation and Visualization\n",
        "\n",
        "Let's evaluate the trained model and visualize the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training curves\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss curve\n",
        "ax1.plot(training_history['loss'], 'b-', label='GRPO Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.grid(True)\n",
        "ax1.legend()\n",
        "\n",
        "# Reward curve\n",
        "ax2.plot(training_history['avg_reward'], 'g-', label='Average Reward')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Reward')\n",
        "ax2.set_title('Average Reward per Epoch')\n",
        "ax2.grid(True)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final average reward: {training_history['avg_reward'][-1]:.3f}\")\n",
        "print(f\"Improvement: {(training_history['avg_reward'][-1] - training_history['avg_reward'][0]) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Test the Trained Model\n",
        "\n",
        "Let's test our GRPO-trained model on some coding problems to see the improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, tokenizer, problem):\n",
        "    \"\"\"Test the model on a single problem\"\"\"\n",
        "    prompt = format_prompt(problem)\n",
        "    \n",
        "    print(f\"Problem: {problem['prompt']}\")\n",
        "    print(f\"Function name: {problem['function_name']}\")\n",
        "    print(\"\\nGenerating solution...\")\n",
        "    \n",
        "    # Generate response\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.1,  # Lower temperature for more deterministic output\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode response\n",
        "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    \n",
        "    print(\"\\nGenerated response:\")\n",
        "    print(response)\n",
        "    \n",
        "    # Extract and test code\n",
        "    code = extract_code(response)\n",
        "    print(\"\\nExtracted code:\")\n",
        "    print(code)\n",
        "    \n",
        "    # Run tests\n",
        "    reward, feedback = execute_code_with_tests(\n",
        "        code,\n",
        "        problem['test_cases'],\n",
        "        problem['function_name']\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nTest results: {feedback}\")\n",
        "    print(f\"Reward: {reward}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    return reward\n",
        "\n",
        "# Test on all problems\n",
        "print(\"Testing GRPO-trained model on all problems...\\n\")\n",
        "test_rewards = []\n",
        "\n",
        "for problem in coding_problems:\n",
        "    reward = test_model(model, tokenizer, problem)\n",
        "    test_rewards.append(reward)\n",
        "    print()\n",
        "\n",
        "# Summary\n",
        "print(\"\\nTest Summary:\")\n",
        "print(f\"Average test reward: {sum(test_rewards) / len(test_rewards):.3f}\")\n",
        "print(f\"Problems solved perfectly: {sum(1 for r in test_rewards if r == 1.0)}/{len(test_rewards)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Key Takeaways and Next Steps\n",
        "\n",
        "#### What We've Learned:\n",
        "\n",
        "1. **GRPO eliminates the value function** - Unlike PPO, GRPO doesn't need a separate critic network, saving memory\n",
        "2. **Group-based advantages** - By generating multiple responses and comparing them, GRPO estimates advantages without a baseline\n",
        "3. **Effective for reasoning tasks** - GRPO is particularly suited for tasks with clear reward signals (like code execution)\n",
        "\n",
        "#### GRPO vs Traditional RL:\n",
        "\n",
        "| Aspect | PPO | GRPO |\n",
        "|--------|-----|------|\n",
        "| Value Function | Required (doubles memory) | Not needed |\n",
        "| Advantage Estimation | Uses value function | Uses group mean |\n",
        "| Sample Efficiency | Lower | Higher (multiple responses) |\n",
        "| Memory Usage | High | Low |\n",
        "| Complexity | More complex | Simpler |\n",
        "\n",
        "#### Next Steps:\n",
        "\n",
        "1. **Scale up the model** - Try with larger base models like Qwen2.5-Math-7B\n",
        "2. **Use better datasets** - HumanEval, MBPP, or custom coding datasets\n",
        "3. **Implement proper tokenization** - Calculate per-token log probabilities for accurate policy ratios\n",
        "4. **Add LoRA** - Combine GRPO with LoRA for even more efficient training\n",
        "5. **Multi-stage training** - Like DeepSeek-R1, use multiple rounds of GRPO with increasing difficulty\n",
        "\n",
        "#### Resources:\n",
        "\n",
        "- [DeepSeekMath Paper](https://arxiv.org/abs/2402.03300) - Original GRPO implementation\n",
        "- [DeepSeek-R1 Paper](https://arxiv.org/abs/2501.12948) - Advanced GRPO application\n",
        "- [HuggingFace TRL](https://github.com/huggingface/trl) - Production GRPO implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Conclusion\n",
        "\n",
        "We've successfully implemented GRPO from scratch! This notebook demonstrates:\n",
        "\n",
        "- How GRPO works conceptually and mathematically\n",
        "- A working implementation that can train models for code generation\n",
        "- The key advantages of GRPO over traditional RL methods\n",
        "\n",
        "GRPO represents a significant advance in training LLMs for reasoning tasks, enabling models like DeepSeek-R1 to achieve remarkable performance while being more efficient than traditional approaches.\n",
        "\n",
        "**Remember**: This is a simplified implementation for learning purposes. Production implementations would include:\n",
        "- Proper per-token probability calculations\n",
        "- Distributed training across multiple GPUs\n",
        "- More sophisticated reward models\n",
        "- Better handling of long sequences\n",
        "- Integration with existing training frameworks\n",
        "\n",
        "Happy coding with GRPO! \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}