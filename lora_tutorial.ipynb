{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9GxiRDNa6RH"
      },
      "source": [
        "# LoRA\n",
        "\n",
        "##Transfer Learning\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805) and [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) popularized the approach of pre-training on large amounts of internet data, then fine-tuning on smaller datasets to perform specialized tasks. This approach of first teaching a model a broad skill (e.g. language modeling), and then re-using that model to learn a more specific task (e.g. sentiment analysis) is called transfer learning.\n",
        "\n",
        "Initially, transfer learning required creating an entirely new model for each new task during fine-tuning. This approach becomes computationally expensive and memory-intensive for larger models.\n",
        "\n",
        "### Adapters\n",
        "\n",
        "[Adapters](https://arxiv.org/abs/1902.00751) introduced a more efficient approach by inserting small bottleneck layers between existing transformer layers. These adapter modules add only a few trainable parameters per task while keeping the original pre-trained weights completely frozen. This way, you only need to train a small number of adapter parameters\n",
        "\n",
        "This reduced the computational load but came with limitations:\n",
        "1. **Inference Latency**: Adapters add extra layers, increasing computational overhead\n",
        "2. **Worse performance**: These methods sometimes underperform full fine-tuning\n",
        "\n",
        "Ideally, we'd like to maintain the same model architecture, as to avoid the latency problem. We'd also like to avoid updating the entire model, as to avoid the high computational cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBR0RPBMbEI6"
      },
      "source": [
        "## Enter LoRA\n",
        "\n",
        "**LoRA (Low-Rank Adaptation)** ([Hu et al., 2021](https://arxiv.org/abs/2106.09685)) provides a solution to this problem.\n",
        "\n",
        "When we fine-tune a model, we freeze the original model and apply weight updates $\\Delta W$. It turns out that $\\Delta W$ is much lower rank than the original weight matrix $W$ - meaning you can express the change without updating every single weight. This is the insight behind LoRA.\n",
        "\n",
        "Instead of updating the full weight matrix:\n",
        "$$W_{new} = W_0 + \\Delta W$$\n",
        "\n",
        "LoRA decomposes the update into two low-rank matrices:\n",
        "$$W_{new} = W_0 + \\Delta W = W_0 + BA$$\n",
        "\n",
        "Where:\n",
        "- $W_0 \\in \\mathbb{R}^{d \\times k}$ is the original frozen weight matrix\n",
        "- $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ are trainable low-rank matrices. These are initialized to zero.\n",
        "- $r \\ll \\min(d, k)$ is the rank (typically 1-64)\n",
        "\n",
        "### Parameter Reduction\n",
        "\n",
        "For a weight matrix of size $d \\times k$:\n",
        "- **Full fine-tuning**: $d \\times k$ parameters\n",
        "- **LoRA**: $r \\times (d + k)$ parameters\n",
        "\n",
        "**Reduction factor**: $\\frac{d \\times k}{r \\times (d + k)}$\n",
        "\n",
        "For large matrices, this can be **100x-1000x fewer parameters**!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KyYNgjJbBpx"
      },
      "source": [
        "## Build: Sentiment Analysis with LoRA\n",
        "\n",
        "We'll implement LoRA for sentiment analysis using the IMDB movie reviews dataset. We'll adapt a pre-trained DistilBERT model (66M params).\n",
        "\n",
        "1. **Baseline**: Test DistilBERT's zero-shot performance on IMDB\n",
        "2. **Resource Analysis**: Examine DistilBERT's architecture and parameter count\n",
        "3. **Implement LoRA**: Build LoRA from scratch and see the math in action\n",
        "4. **Training & Comparison**: Compare LoRA vs full fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cgoKljcbbj6"
      },
      "source": [
        "### 1. Setup and Data Loading\n",
        "\n",
        "First, let's install the required packages and load our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XoQM9q7GayEF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.46.1)\n",
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.1.0)\n",
            "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.1)\n",
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.7.0)\n",
            "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.9.2)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.26.3)\n",
            "Requirement already satisfied: accelerate in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.8.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.2.0)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: psutil in /Users/kylerwang/Library/Python/3.12/lib/python/site-packages (from accelerate) (6.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets torch scikit-learn matplotlib numpy accelerate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    DistilBertModel,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FU3DJJoUayEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading IMDB dataset...\n",
            "loaded 5000 training samples and 1000 test samples\n",
            "\n",
            "Sample reviews:\n",
            "\n",
            "Positive: There is no relation at all between Fortier and Profiler but the fact that both are police series ab...\n",
            "\n",
            "Positive: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. T...\n",
            "\n",
            "Negative: George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly...\n",
            "\n",
            "Positive: In the process of trying to establish the audiences' empathy with Jake Roedel (Tobey Maguire) the fi...\n",
            "\n",
            "Negative: Yeh, I know -- you're quivering with excitement. Well, *The Secret Lives of Dentists* will not upset...\n"
          ]
        }
      ],
      "source": [
        "# Load IMDB dataset\n",
        "print(\"Loading IMDB dataset...\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# using subset for faster experimentation (5k samples)\n",
        "SUBSET_SIZE = 5000\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(SUBSET_SIZE))\n",
        "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))  # 1k for testing\n",
        "\n",
        "print(f\"loaded {len(train_dataset)} training samples and {len(test_dataset)} test samples\")\n",
        "\n",
        "# look at a few examples\n",
        "print(\"\\nSample reviews:\")\n",
        "for i in range(5):\n",
        "    review = train_dataset[i]\n",
        "    label = \"Positive\" if review[\"label\"] == 1 else \"Negative\"\n",
        "    text_preview = review[\"text\"][:100] + \"...\" if len(review[\"text\"]) > 100 else review[\"text\"]\n",
        "    print(f\"\\n{label}: {text_preview}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFue-RQZbid0"
      },
      "source": [
        "### 2. Baseline: Pre-trained DistilBERT Performance\n",
        "\n",
        "Let's see how DistilBERT performs on sentiment analysis without any task-specific training. We'll use DistilBERT with a randomly initialized classification head to show that the pre-trained model alone cannot perform sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_pPlE7dyayEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading DistilBERT...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BASELINE EVALUATION ===\n",
            "Evaluating model on 200 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:07<00:00, 27.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline accuracy: 0.575\n",
            "(This is essentially random - the classification head is randomly initialized!)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# load pre-trained DistilBERT model and tokenizer\n",
        "print(\"Loading DistilBERT...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# load DistilBERT with classification head (untrained)\n",
        "baseline_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased', \n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "def evaluate_model(model, test_data, sample_size=1000):\n",
        "    \"\"\"\n",
        "    Standard evaluation function for any DistilBERT model\n",
        "    This function will be reused across baseline, fine-tuning, and LoRA\n",
        "    \"\"\"\n",
        "    # Move model to CPU for evaluation to avoid device issues\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(f\"Evaluating model on {sample_size} samples...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(min(sample_size, len(test_data)))):\n",
        "            text = test_data[i][\"text\"]\n",
        "            true_label = test_data[i][\"label\"]\n",
        "            \n",
        "            # Tokenize and get model prediction (inputs will be on CPU)\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
        "                             max_length=128, padding=True)\n",
        "            \n",
        "            outputs = model(**inputs)\n",
        "            predicted_label = outputs.logits.argmax(-1).item()\n",
        "\n",
        "            if predicted_label == true_label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Test baseline performance (untrained model)\n",
        "print(\"=== BASELINE EVALUATION ===\")\n",
        "baseline_acc = evaluate_model(baseline_model, test_dataset, sample_size=200)\n",
        "print(f\"Baseline accuracy: {baseline_acc:.3f}\")\n",
        "print(\"(This is essentially random - the classification head is randomly initialized!)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Maz-0fD1blDb"
      },
      "source": [
        "### 3. Resource Analysis: Understanding DistilBERT\n",
        "\n",
        "Before we implement LoRA, let's examine DistilBERT's architecture and see exactly where our parameters are and what we'd be updating.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JHUkFsuGayEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DistilBERT: 66,955,010 parameters (~255 MB)\n",
            "hidden_size: 768\n",
            "num_layers: 6\n",
            "Attention matrices: 14,155,776 parameters (21% of model)\n"
          ]
        }
      ],
      "source": [
        "# DistilBERT Parameter Overview\n",
        "total_params = sum(p.numel() for p in baseline_model.parameters())\n",
        "print(f\"DistilBERT: {total_params:,} parameters (~255 MB)\")\n",
        "print(f\"hidden_size: {baseline_model.config.dim}\")  # 768 for DistilBERT\n",
        "print(f\"num_layers: {len(baseline_model.distilbert.transformer.layer)}\")  # 6 layers\n",
        "\n",
        "\n",
        "# Focus on attention matrices (where LoRA works)\n",
        "attn_layer = baseline_model.distilbert.transformer.layer[0].attention\n",
        "attn_matrix_size = attn_layer.q_lin.weight.numel()\n",
        "total_attn_params = attn_matrix_size * 4 * 6  # 4 matrices × 6 layers\n",
        "print(f\"Attention matrices: {total_attn_params:,} parameters (21% of model)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKdOJiX2brk6"
      },
      "source": [
        "### 4. Optional: Full Fine-Tuning (⚠️ Time-Intensive)\n",
        "\n",
        "This section contains pre-written code for full fine-tuning comparison. This will provide an additional baseline to compare against for LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ckYskD4iayEH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting full fine-tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 50/626 [00:11<02:15,  4.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6712, 'grad_norm': 2.6215243339538574, 'learning_rate': 2.5e-05, 'epoch': 0.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 100/626 [00:23<02:01,  4.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4834, 'grad_norm': 12.022976875305176, 'learning_rate': 5e-05, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 150/626 [00:35<01:50,  4.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4123, 'grad_norm': 11.040996551513672, 'learning_rate': 4.524714828897338e-05, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 200/626 [00:47<01:41,  4.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4378, 'grad_norm': 4.789593696594238, 'learning_rate': 4.0494296577946774e-05, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 32%|███▏      | 200/626 [00:51<01:41,  4.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3823752701282501, 'eval_accuracy': 0.828, 'eval_runtime': 3.7727, 'eval_samples_per_second': 265.059, 'eval_steps_per_second': 16.699, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 250/626 [01:02<01:26,  4.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4001, 'grad_norm': 5.0064897537231445, 'learning_rate': 3.574144486692015e-05, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 300/626 [01:14<01:15,  4.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4041, 'grad_norm': 7.163127899169922, 'learning_rate': 3.098859315589354e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 350/626 [01:25<01:04,  4.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.273, 'grad_norm': 4.326177597045898, 'learning_rate': 2.6235741444866924e-05, 'epoch': 1.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 400/626 [01:37<00:54,  4.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2236, 'grad_norm': 2.5286529064178467, 'learning_rate': 2.1482889733840306e-05, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 64%|██████▍   | 400/626 [01:41<00:54,  4.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.44193750619888306, 'eval_accuracy': 0.847, 'eval_runtime': 3.9083, 'eval_samples_per_second': 255.868, 'eval_steps_per_second': 16.12, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 450/626 [01:53<00:40,  4.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2529, 'grad_norm': 0.29494351148605347, 'learning_rate': 1.673003802281369e-05, 'epoch': 1.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|███████▉  | 500/626 [02:05<00:29,  4.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2216, 'grad_norm': 4.211336135864258, 'learning_rate': 1.1977186311787073e-05, 'epoch': 1.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 550/626 [02:16<00:18,  4.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2167, 'grad_norm': 8.358245849609375, 'learning_rate': 7.224334600760456e-06, 'epoch': 1.76}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 600/626 [02:28<00:06,  4.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2194, 'grad_norm': 6.466604709625244, 'learning_rate': 2.4714828897338406e-06, 'epoch': 1.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 96%|█████████▌| 600/626 [02:32<00:06,  4.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.40997517108917236, 'eval_accuracy': 0.847, 'eval_runtime': 3.7747, 'eval_samples_per_second': 264.924, 'eval_steps_per_second': 16.69, 'epoch': 1.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 626/626 [02:38<00:00,  3.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 158.4878, 'train_samples_per_second': 63.096, 'train_steps_per_second': 3.95, 'train_loss': 0.3458256378722267, 'epoch': 2.0}\n",
            "Full fine-tuning completed in 158.8 seconds\n",
            "\n",
            "=== FULL FINE-TUNING EVALUATION ===\n",
            "Evaluating model on 1000 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:39<00:00, 25.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full fine-tuning accuracy: 0.854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Full Fine-tuning Implementation\n",
        "print(\"Starting full fine-tuning...\")\n",
        "\n",
        "# Use a copy of the baseline model for consistency\n",
        "import copy\n",
        "classification_model = copy.deepcopy(baseline_model)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Training arguments - optimized for speed\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,  # Reduced for time\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"no\",  # Don't save to reduce overhead\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=classification_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=lambda eval_pred: {\n",
        "        'accuracy': accuracy_score(eval_pred.label_ids, eval_pred.predictions.argmax(-1))\n",
        "    }\n",
        ")\n",
        "\n",
        "# Train\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "full_finetuning_time = time.time() - start_time\n",
        "\n",
        "print(f\"Full fine-tuning completed in {full_finetuning_time:.1f} seconds\")\n",
        "\n",
        "# Evaluate using the same function as baseline (for fair comparison)\n",
        "print(\"\\n=== FULL FINE-TUNING EVALUATION ===\")\n",
        "full_finetuning_accuracy = evaluate_model(classification_model, test_dataset, sample_size=1000)\n",
        "print(f\"Full fine-tuning accuracy: {full_finetuning_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub3qabMgayEH",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 5. LoRA Implementation: Step by Step\n",
        "\n",
        "Let's implement LoRA from scratch and see the mathematical decomposition in action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx4OWU_EayEH",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.1 The Math: Low-Rank Decomposition\n",
        "\n",
        "Recall our goal: instead of updating the full weight matrix W, we want to learn a low-rank update:\n",
        "\n",
        "**W_new = W_0 + ΔW = W_0 + B @ A**\n",
        "\n",
        "Where:\n",
        "- W_0: Original frozen weights (768 × 768 for DistilBERT attention)\n",
        "- B: Trainable matrix (768 × r)\n",
        "- A: Trainable matrix (r × 768)\n",
        "- r: Rank (we'll use r=16)\n",
        "\n",
        "Parameter savings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uKXgJBxjayEH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA Decomposition:\n",
            "W_new = W₀ + B@A\n",
            "Where B: 768×16, A: 16×768\n",
            "\n",
            "Parameter comparison per matrix:\n",
            "Full update: 589,824\n",
            "LoRA update: 24,576\n",
            "Reduction: 24x\n",
            "\n",
            "DistilBERT total: 589,824 LoRA parameters added\n"
          ]
        }
      ],
      "source": [
        "# LoRA parameter calculation\n",
        "d, k = 768, 768  # DistilBERT attention matrix dimensions\n",
        "r = 16          # LoRA rank\n",
        "\n",
        "print(\"LoRA Decomposition:\")\n",
        "print(f\"W_new = W₀ + B@A\")\n",
        "print(f\"Where B: {d}×{r}, A: {r}×{k}\")\n",
        "print()\n",
        "\n",
        "# Parameter counts\n",
        "full_params = d * k\n",
        "lora_params = r * (d + k)\n",
        "\n",
        "print(\"Parameter comparison per matrix:\")\n",
        "print(f\"Full update: {full_params:,}\")\n",
        "print(f\"LoRA update: {lora_params:,}\")\n",
        "print(f\"Reduction: {full_params // lora_params}x\")\n",
        "print()\n",
        "\n",
        "# Total for DistilBERT\n",
        "num_matrices = 4 * 6  # Q,K,V,O × 6 layers\n",
        "total_lora = lora_params * num_matrices\n",
        "print(f\"DistilBERT total: {total_lora:,} LoRA parameters added\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "VfkSomWXayEI",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.2 LoRA Layer Implementation\n",
        "\n",
        "Build our LoRA layer from scratch:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "NEkArprYayEI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing LoRA layer...\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "Forward pass successful!\n",
            "Output shapes match: True\n",
            "Difference (should be ~0 initially): 0.000000\n"
          ]
        }
      ],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    LoRA (Low-Rank Adaptation) layer\n",
        "\n",
        "    Implements: output = input @ (W_0 + B @ A)\n",
        "    Where W_0 is frozen, and B, A are trainable low-rank matrices\n",
        "    \"\"\"\n",
        "    def __init__(self, original_layer, rank=16, alpha=16):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store the original layer (frozen)\n",
        "        self.original_layer = original_layer\n",
        "        self.original_layer.requires_grad_(False)  # Freeze original weights\n",
        "\n",
        "        # Get dimensions\n",
        "        if hasattr(original_layer, 'in_features'):  # Linear layer\n",
        "            in_features = original_layer.in_features\n",
        "            out_features = original_layer.out_features\n",
        "        else:\n",
        "            raise ValueError(\"LoRA currently supports nn.Linear layers\")\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank  # Scaling factor\n",
        "\n",
        "        # A: normal initialization (like original paper)\n",
        "        # B: zero initialization (so ΔW = B@A starts at zero)\n",
        "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.1)\n",
        "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
        "\n",
        "        print(f\"Created LoRA layer: {out_features}×{in_features} → rank {rank}\")\n",
        "        print(f\"   Original params: {in_features * out_features:,}\")\n",
        "        print(f\"   LoRA params: {rank * (in_features + out_features):,}\")\n",
        "        print(f\"   Reduction: {(in_features * out_features) / (rank * (in_features + out_features)):.1f}x\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Original computation: x @ W_0^T + b_0\n",
        "        original_output = self.original_layer(x)\n",
        "\n",
        "        # LoRA computation: x @ (B @ A)^T = x @ A^T @ B^T\n",
        "        # Note: we need to transpose because PyTorch Linear does x @ W^T\n",
        "        lora_output = x @ self.lora_A.T @ self.lora_B.T * self.scaling\n",
        "\n",
        "        # Combine: W_new = W_0 + B @ A\n",
        "        return original_output + lora_output\n",
        "\n",
        "    def get_lora_parameters(self):\n",
        "        \"\"\"Return only the LoRA parameters for optimization\"\"\"\n",
        "        return [self.lora_A, self.lora_B]\n",
        "\n",
        "# Test our LoRA layer\n",
        "print(\"Testing LoRA layer...\")\n",
        "test_linear = nn.Linear(768, 768)\n",
        "test_input = torch.randn(1, 10, 768)  # Batch size 1, sequence length 10\n",
        "\n",
        "# Create LoRA version\n",
        "lora_layer = LoRALayer(test_linear, rank=16)\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    original_output = test_linear(test_input)\n",
        "    lora_output = lora_layer(test_input)\n",
        "\n",
        "print(f\"Forward pass successful!\")\n",
        "print(f\"Output shapes match: {original_output.shape == lora_output.shape}\")\n",
        "print(f\"Difference (should be ~0 initially): {(original_output - lora_output).abs().max().item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "BYs5bIlhayEI",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.3 Applying LoRA to DistilBERT\n",
        "\n",
        "Apply our LoRA layers to the attention matrices in DistilBERT:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MPjho8c5ayEI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating LoRA model from baseline...\n",
            "Applying LoRA (rank=16) to DistilBERT attention layers...\n",
            "\n",
            "Layer 0:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 1:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 2:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 3:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 4:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 5:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "LoRA Application Summary:\n",
            "Original attention parameters: 14,174,208\n",
            "LoRA parameters added: 18,816\n",
            "Reduction factor: 753.3x\n",
            "\n",
            "Ready for LoRA training with 48 trainable parameters!\n"
          ]
        }
      ],
      "source": [
        "def apply_lora_to_distilbert(model, rank=16):\n",
        "    \"\"\"Apply LoRA to all attention layers in DistilBERT\"\"\"\n",
        "\n",
        "    print(f\"Applying LoRA (rank={rank}) to DistilBERT attention layers...\")\n",
        "\n",
        "    lora_params = []\n",
        "    original_params = 0\n",
        "    lora_param_count = 0\n",
        "\n",
        "    # Apply LoRA to each transformer layer's attention\n",
        "    for layer_idx, transformer_layer in enumerate(model.distilbert.transformer.layer):\n",
        "        attention = transformer_layer.attention\n",
        "\n",
        "        print(f\"\\nLayer {layer_idx}:\")\n",
        "\n",
        "        # Apply to Query, Key, Value, and Output projections\n",
        "        for name, linear_layer in [\n",
        "            ('q_lin', attention.q_lin),\n",
        "            ('k_lin', attention.k_lin),\n",
        "            ('v_lin', attention.v_lin),\n",
        "            ('out_lin', attention.out_lin)\n",
        "        ]:\n",
        "            # Count original parameters\n",
        "            orig_params = sum(p.numel() for p in linear_layer.parameters())\n",
        "            original_params += orig_params\n",
        "\n",
        "            # Replace with LoRA layer\n",
        "            lora_layer = LoRALayer(linear_layer, rank=rank)\n",
        "            setattr(attention, name, lora_layer)\n",
        "\n",
        "            # Collect LoRA parameters\n",
        "            lora_params.extend(lora_layer.get_lora_parameters())\n",
        "            lora_param_count += len(lora_layer.get_lora_parameters()[0]) + len(lora_layer.get_lora_parameters()[1])\n",
        "\n",
        "            print(f\"  {name}: LoRA applied\")\n",
        "\n",
        "    print(f\"\\nLoRA Application Summary:\")\n",
        "    print(f\"Original attention parameters: {original_params:,}\")\n",
        "    print(f\"LoRA parameters added: {lora_param_count:,}\")\n",
        "    print(f\"Reduction factor: {original_params / lora_param_count:.1f}x\")\n",
        "\n",
        "    return lora_params\n",
        "\n",
        "# Use a copy of the baseline model for consistency  \n",
        "print(\"Creating LoRA model from baseline...\")\n",
        "lora_model = copy.deepcopy(baseline_model)\n",
        "\n",
        "# Apply LoRA\n",
        "lora_parameters = apply_lora_to_distilbert(lora_model, rank=16)\n",
        "\n",
        "print(f\"\\nReady for LoRA training with {len(lora_parameters):,} trainable parameters!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "pVVOnYz2ayEI",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.4 LoRA Training and Results\n",
        "\n",
        "Now let's train our LoRA model and compare the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s0jauWayEI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up LoRA training...\n",
            "Using pre-tokenized datasets from full fine-tuning section...\n",
            "Training LoRA for 2 epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   0%|          | 0/313 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# LoRA Training setup\n",
        "print(\"Setting up LoRA training...\")\n",
        "\n",
        "# Only optimize LoRA parameters + classifier head\n",
        "optimizer = torch.optim.AdamW([\n",
        "    *lora_parameters,  # LoRA parameters\n",
        "    *lora_model.classifier.parameters()  # Classification head\n",
        "], lr=5e-4, weight_decay=0.01)\n",
        "\n",
        "# Simple training loop using pre-tokenized data (for fair comparison)\n",
        "def train_lora_model(model, train_data, test_data, epochs=2):\n",
        "    model.train()\n",
        "\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    print(f\"Training LoRA for {epochs} epochs...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Training\n",
        "        for i in tqdm(range(0, len(train_data), 16), desc=f\"Epoch {epoch+1}\"):\n",
        "            # Get batch from pre-tokenized dataset\n",
        "            batch_indices = list(range(i, min(i+16, len(train_data))))\n",
        "            \n",
        "            # Extract pre-tokenized inputs and labels\n",
        "            input_ids = torch.stack([torch.tensor(train_data[j]['input_ids']) for j in batch_indices])\n",
        "            attention_mask = torch.stack([torch.tensor(train_data[j]['attention_mask']) for j in batch_indices])\n",
        "            labels = torch.tensor([train_data[j]['label'] for j in batch_indices])\n",
        "            \n",
        "            inputs = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask\n",
        "            }\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, min(500, len(test_data)), 16):  # Quick eval\n",
        "                # Get batch from pre-tokenized dataset\n",
        "                batch_indices = list(range(i, min(i+16, len(test_data))))\n",
        "                \n",
        "                input_ids = torch.stack([torch.tensor(test_data[j]['input_ids']) for j in batch_indices])\n",
        "                attention_mask = torch.stack([torch.tensor(test_data[j]['attention_mask']) for j in batch_indices])\n",
        "                labels = torch.tensor([test_data[j]['label'] for j in batch_indices])\n",
        "                \n",
        "                inputs = {\n",
        "                    'input_ids': input_ids,\n",
        "                    'attention_mask': attention_mask\n",
        "                }\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                predictions = outputs.logits.argmax(-1)\n",
        "\n",
        "                correct += (predictions == labels).sum().item()\n",
        "                total += len(labels)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        test_accuracies.append(accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.3f}\")\n",
        "        model.train()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    return train_losses, test_accuracies, training_time\n",
        "\n",
        "# Train LoRA model using the SAME pre-tokenized data as full fine-tuning\n",
        "# This makes for a much fairer speed comparison!\n",
        "print(\"Using pre-tokenized datasets from full fine-tuning section...\")\n",
        "losses, accuracies, lora_time = train_lora_model(\n",
        "    lora_model, tokenized_train, tokenized_test, epochs=2\n",
        ")\n",
        "\n",
        "# Final evaluation\n",
        "lora_model.eval()\n",
        "final_accuracy = accuracies[-1] if accuracies else 0\n",
        "\n",
        "print(f\"\\nLoRA Training Complete!\")\n",
        "print(f\"Training time: {lora_time:.1f} seconds\")\n",
        "print(f\"Final accuracy: {final_accuracy:.3f}\")\n",
        "\n",
        "# Compare with full fine-tuning\n",
        "print(f\"\\nCOMPARISON SUMMARY:\")\n",
        "print(f\"{'Method':<20} {'Accuracy':<10} {'Time (s)':<10} {'Parameters':<15}\")\n",
        "print(f\"{'-'*55}\")\n",
        "print(f\"{'Baseline':<20} {baseline_acc:<10.3f} {'N/A':<10} {'0':<15}\")\n",
        "print(f\"{'LoRA':<20} {final_accuracy:<10.3f} {lora_time:<10.1f} {len(lora_parameters)*2:,}{'':>5}\")\n",
        "print(f\"{'Full Fine-tuning':<20} {full_finetuning_accuracy:<10.3f} {full_finetuning_time:<10.1f} {'66,955,010':<15}\")\n",
        "\n",
        "print(f\"\\nKey Takeaways:\")\n",
        "print(f\"• LoRA achieved {final_accuracy:.1%} accuracy with {(len(lora_parameters)*2/66955010)*100:.2f}% of parameters\")\n",
        "print(f\"• LoRA was {full_finetuning_time/lora_time:.1f}x faster to train\")\n",
        "print(f\"• LoRA performance: {final_accuracy/full_finetuning_accuracy*100:.1f}% of full fine-tuning accuracy\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses, 'b-', label='LoRA Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(accuracies, 'g-', label='LoRA Accuracy')\n",
        "plt.axhline(y=full_finetuning_accuracy, color='r', linestyle='--', label='Full Fine-tuning')\n",
        "plt.axhline(y=baseline_acc, color='gray', linestyle=':', label='Baseline')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCongratulations! You've successfully implemented and trained LoRA from scratch!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
