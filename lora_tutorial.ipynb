{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# LoRA\n",
        "\n",
        "## Context\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805) and [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) popularized the approach of pre-training on large amounts of internet data, then fine-tuning on smaller datasets to perform specialized tasks. This approach of first teaching a model a broad skill (e.g. language modeling), and then re-using that model to learn a more specific task (e.g. sentiment analysis) is called transfer learning.\n",
        "\n",
        "Initially, transfer learning required creating an entirely new model for each new task during fine-tuning. This approach becomes computationally expensive and memory-intensive for larger models. \n",
        "\n",
        "### Enter Adapters\n",
        "\n",
        "[Adapters](https://arxiv.org/abs/1902.00751) introduced a more efficient approach by inserting small bottleneck layers between existing transformer layers. These adapter modules add only a few trainable parameters per task while keeping the original pre-trained weights completely frozen.\n",
        "\n",
        "Adapters work by:\n",
        "- Keeping the original pre-trained weights **frozen**\n",
        "- Adding small bottleneck layers between existing layers\n",
        "- Only training these new adapter parameters\n",
        "### Other Parameter-Efficient Approaches\n",
        "\n",
        "Other approaches like prefix tuning emerged, which optimize trainable prompt tokens instead of model weights.\n",
        "\n",
        "These methods were more efficient than full fine-tuning, but they had limitations:\n",
        "1. **Inference Latency**: Adapters add extra layers, increasing computational overhead\n",
        "2. **Worse performance**: These methods sometimes underperform full fine-tuning\n",
        "\n",
        "Ideally, we'd like to maintain the same model architecture as the original, as to avoid the latency problem. We'd also like to avoid updating the entire model, as to avoid the high computational cost. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## Enter LoRA \n",
        "\n",
        "**LoRA (Low-Rank Adaptation)** ([Hu et al., 2021](https://arxiv.org/abs/2106.09685)) provides a solution to this problem.\n",
        "\n",
        "When we fine-tune a model, we freese the original model and apply weight updates $\\Delta W$. It turns out that $\\Delta W$ is much lower rank than the original weight matrix $W$ - meaning you can express the change without updating every single weight. This is the insight behind LoRA. \n",
        "\n",
        "Instead of updating the full weight matrix:\n",
        "$$W_{new} = W_0 + \\Delta W$$\n",
        "\n",
        "LoRA decomposes the update into two low-rank matrices:\n",
        "$$W_{new} = W_0 + \\Delta W = W_0 + BA$$\n",
        "\n",
        "Where:\n",
        "- $W_0 \\in \\mathbb{R}^{d \\times k}$ is the original frozen weight matrix\n",
        "- $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ are trainable low-rank matrices. These are initialized to zero.\n",
        "- $r \\ll \\min(d, k)$ is the rank (typically 1-64)\n",
        "\n",
        "### Parameter Reduction\n",
        "\n",
        "For a weight matrix of size $d \\times k$:\n",
        "- **Full fine-tuning**: $d \\times k$ parameters\n",
        "- **LoRA**: $r \\times (d + k)$ parameters\n",
        "\n",
        "**Reduction factor**: $\\frac{d \\times k}{r \\times (d + k)}$\n",
        "\n",
        "For large matrices, this can be **100x-1000x fewer parameters**!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## Build: Sentiment Analysis with LoRA\n",
        "\n",
        "We'll implement LoRA for sentiment analysis using the IMDB movie reviews dataset. We'll adapt a pre-trained DistilBERT model (66M params). \n",
        "\n",
        "1. **Baseline**: Test DistilBERT's zero-shot performance on IMDB\n",
        "2. **Resource Analysis**: Examine DistilBERT's architecture and parameter count\n",
        "3. **Implement LoRA**: Build LoRA from scratch and see the math in action\n",
        "4. **Training & Comparison**: Compare LoRA vs full fine-tuning.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1. Setup and Data Loading\n",
        "\n",
        "First, let's install the required packages and load our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers datasets torch scikit-learn matplotlib numpy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    DistilBertTokenizer, \n",
        "    DistilBertForSequenceClassification,\n",
        "    DistilBertModel,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load IMDB dataset\n",
        "print(\"Loading IMDB dataset...\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# using subset for faster experimentation (5k samples)\n",
        "SUBSET_SIZE = 5000\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(SUBSET_SIZE))\n",
        "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))  # 1k for testing\n",
        "\n",
        "print(f\"loaded {len(train_dataset)} training samples and {len(test_dataset)} test samples\")\n",
        "\n",
        "# look at a few examples\n",
        "print(\"\\nSample reviews:\")\n",
        "for i in range(2):\n",
        "    review = train_dataset[i]\n",
        "    label = \"Positive\" if review[\"label\"] == 1 else \"Negative\"\n",
        "    text_preview = review[\"text\"][:100] + \"...\" if len(review[\"text\"]) > 100 else review[\"text\"]\n",
        "    print(f\"\\n{label}: {text_preview}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2. Baseline: Pre-trained DistilBERT Performance\n",
        "\n",
        "Let's see how DistilBERT performs on sentiment analysis without any fine-tuning. We'll use a simple approach: if the model assigns higher probability to positive sentiment words, we'll classify it as positive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "print(\"Loading DistilBERT...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# For zero-shot sentiment analysis, we'll use a simple approach:\n",
        "# Look at the model's hidden representations and see if we can distinguish sentiment\n",
        "def simple_sentiment_baseline(texts, sample_size=100):\n",
        "    \"\"\"\n",
        "    Simple baseline: use DistilBERT's [CLS] token representation \n",
        "    to see if there's any inherent sentiment signal\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    print(f\"Testing baseline on {sample_size} samples...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(min(sample_size, len(texts)))):\n",
        "            text = texts[i][\"text\"]\n",
        "            true_label = texts[i][\"label\"]\n",
        "            \n",
        "            # Tokenize\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
        "                             max_length=128, padding=True)\n",
        "            \n",
        "            # Get DistilBERT representation\n",
        "            outputs = model(**inputs)\n",
        "            cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "            \n",
        "            # Simple heuristic: sum of embedding values\n",
        "            # (This is obviously not a good approach, but shows the baseline)\n",
        "            sentiment_score = cls_embedding.sum().item()\n",
        "            predicted_label = 1 if sentiment_score > 0 else 0\n",
        "            \n",
        "            if predicted_label == true_label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Test baseline performance\n",
        "baseline_acc = simple_sentiment_baseline(test_dataset, sample_size=200)\n",
        "print(f\"\\nBaseline accuracy: {baseline_acc:.3f}\")\n",
        "print(\"(This is essentially random - DistilBERT wasn't trained for sentiment analysis!)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3. Resource Analysis: Understanding DistilBERT\n",
        "\n",
        "Before we implement LoRA, let's examine DistilBERT's architecture and see exactly where our parameters are and what we'd be updating.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze DistilBERT's parameters\n",
        "def analyze_model_parameters(model):\n",
        "    \"\"\"Analyze the parameter distribution in DistilBERT\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(f\"DistilBERT Parameter Analysis:\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Model size: ~{total_params * 4 / 1024**2:.1f} MB (float32)\")\n",
        "    \n",
        "    # Look at specific layer types\n",
        "    layer_counts = {}\n",
        "    attention_params = 0\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        layer_type = name.split('.')[0] if '.' in name else name\n",
        "        \n",
        "        if layer_type not in layer_counts:\n",
        "            layer_counts[layer_type] = 0\n",
        "        layer_counts[layer_type] += param.numel()\n",
        "        \n",
        "        # Count attention parameters specifically\n",
        "        if 'attention' in name and any(x in name for x in ['q_lin', 'k_lin', 'v_lin', 'out_lin']):\n",
        "            attention_params += param.numel()\n",
        "    \n",
        "    print(f\"\\nParameter breakdown by component:\")\n",
        "    for layer, count in sorted(layer_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {layer}: {count:,} parameters ({count/total_params*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nAttention mechanism parameters: {attention_params:,} ({attention_params/total_params*100:.1f}%)\")\n",
        "    \n",
        "    return total_params, attention_params\n",
        "\n",
        "# Analyze our model\n",
        "total_params, attention_params = analyze_model_parameters(model)\n",
        "\n",
        "# Look at a specific attention layer to understand the matrices we'll modify\n",
        "print(f\"\\nExamining a single attention layer:\")\n",
        "first_attn = model.transformer.layer[0].attention\n",
        "print(f\"Query matrix shape: {first_attn.q_lin.weight.shape}\")\n",
        "print(f\"Key matrix shape: {first_attn.k_lin.weight.shape}\")  \n",
        "print(f\"Value matrix shape: {first_attn.v_lin.weight.shape}\")\n",
        "print(f\"Output matrix shape: {first_attn.out_lin.weight.shape}\")\n",
        "\n",
        "# This is where LoRA will make a big difference\n",
        "single_attn_params = sum(p.numel() for p in first_attn.parameters())\n",
        "print(f\"Parameters in one attention layer: {single_attn_params:,}\")\n",
        "print(f\"DistilBERT has 6 such layers = {single_attn_params * 6:,} attention parameters total\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4. Optional: Full Fine-Tuning (⚠️ Time-Intensive)\n",
        "\n",
        "This section contains pre-written code for full fine-tuning comparison. **Skip this if you want to save time** - we'll simulate the results for comparison with LoRA.\n",
        "\n",
        "**Estimated time:**\n",
        "- GPU: ~20-30 minutes  \n",
        "- CPU: ~2-3 hours\n",
        "\n",
        "<details>\n",
        "<summary>Click to expand full fine-tuning code</summary>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full Fine-tuning Implementation (OPTIONAL - takes longer)\n",
        "# Set this to True only if you want to run full fine-tuning\n",
        "RUN_FULL_FINETUNING = False  # Change to True to run\n",
        "\n",
        "if RUN_FULL_FINETUNING:\n",
        "    print(\"Starting full fine-tuning...\")\n",
        "    \n",
        "    # Create a classification model\n",
        "    classification_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        'distilbert-base-uncased', \n",
        "        num_labels=2\n",
        "    )\n",
        "    \n",
        "    # Tokenize dataset\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
        "    \n",
        "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "    \n",
        "    # Training arguments - optimized for speed\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=2,  # Reduced for time\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=50,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=200,\n",
        "        save_strategy=\"no\",  # Don't save to reduce overhead\n",
        "    )\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=classification_model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_test,\n",
        "        compute_metrics=lambda eval_pred: {\n",
        "            'accuracy': accuracy_score(eval_pred.label_ids, eval_pred.predictions.argmax(-1))\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    full_finetuning_time = time.time() - start_time\n",
        "    \n",
        "    # Evaluate\n",
        "    full_finetuning_results = trainer.evaluate()\n",
        "    print(f\"Full fine-tuning completed in {full_finetuning_time:.1f} seconds\")\n",
        "    print(f\"Full fine-tuning accuracy: {full_finetuning_results['eval_accuracy']:.3f}\")\n",
        "    \n",
        "else:\n",
        "    # Simulated results for comparison (typical performance)\n",
        "    print(\"Skipping full fine-tuning (RUN_FULL_FINETUNING=False)\")\n",
        "    print(\"Typical full fine-tuning results on this task:\")\n",
        "    print(\"   - Accuracy: ~0.85-0.90\")\n",
        "    print(\"   - Training time: 20-30 min (GPU) / 2-3 hours (CPU)\")\n",
        "    print(\"   - Parameters updated: 66,955,010 (100%)\")\n",
        "    \n",
        "    # Store simulated results for later comparison\n",
        "    full_finetuning_time = 1800  # 30 minutes\n",
        "    full_finetuning_accuracy = 0.87\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "</details>\n",
        "\n",
        "### 5. LoRA Implementation: Step by Step\n",
        "\n",
        "Let's implement LoRA from scratch and see the mathematical decomposition in action.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.1 The Math: Low-Rank Decomposition\n",
        "\n",
        "Recall our goal: instead of updating the full weight matrix W, we want to learn a low-rank update:\n",
        "\n",
        "**W_new = W_0 + ΔW = W_0 + B @ A**\n",
        "\n",
        "Where:\n",
        "- W_0: Original frozen weights (768 × 768 for DistilBERT attention)  \n",
        "- B: Trainable matrix (768 × r)\n",
        "- A: Trainable matrix (r × 768)\n",
        "- r: Rank (we'll use r=16)\n",
        "\n",
        "Parameter savings:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate parameter savings for LoRA\n",
        "d, k = 768, 768  # DistilBERT attention matrix dimensions\n",
        "r = 16  # LoRA rank\n",
        "\n",
        "# Full matrix update\n",
        "full_params = d * k\n",
        "print(f\"Parameter Comparison:\")\n",
        "print(f\"Full fine-tuning: {full_params:,} parameters per matrix\")\n",
        "\n",
        "# LoRA update  \n",
        "lora_params = r * (d + k)\n",
        "print(f\"LoRA (rank {r}): {lora_params:,} parameters per matrix\")\n",
        "\n",
        "# Reduction factor\n",
        "reduction = full_params / lora_params\n",
        "print(f\"Reduction factor: {reduction:.1f}x fewer parameters\")\n",
        "\n",
        "# For all attention matrices in DistilBERT\n",
        "num_attention_matrices = 4 * 6  # 4 matrices (Q,K,V,O) × 6 layers\n",
        "total_full = full_params * num_attention_matrices\n",
        "total_lora = lora_params * num_attention_matrices\n",
        "\n",
        "print(f\"\\nFor all DistilBERT attention layers:\")\n",
        "print(f\"Full fine-tuning: {total_full:,} parameters\")\n",
        "print(f\"LoRA: {total_lora:,} parameters\")\n",
        "print(f\"Total reduction: {total_full / total_lora:.1f}x\")\n",
        "\n",
        "# Memory calculation\n",
        "print(f\"\\nMemory impact:\")\n",
        "print(f\"LoRA adds only: {total_lora * 4 / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.2 LoRA Layer Implementation\n",
        "\n",
        "Build our LoRA layer from scratch:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    LoRA (Low-Rank Adaptation) layer\n",
        "    \n",
        "    Implements: output = input @ (W_0 + B @ A)\n",
        "    Where W_0 is frozen, and B, A are trainable low-rank matrices\n",
        "    \"\"\"\n",
        "    def __init__(self, original_layer, rank=16, alpha=16):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Store the original layer (frozen)\n",
        "        self.original_layer = original_layer\n",
        "        self.original_layer.requires_grad_(False)  # Freeze original weights\n",
        "        \n",
        "        # Get dimensions\n",
        "        if hasattr(original_layer, 'in_features'):  # Linear layer\n",
        "            in_features = original_layer.in_features\n",
        "            out_features = original_layer.out_features\n",
        "        else:\n",
        "            raise ValueError(\"LoRA currently supports nn.Linear layers\")\n",
        "        \n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank  # Scaling factor\n",
        "        \n",
        "        # Initialize LoRA matrices\n",
        "        # A: Normal initialization (like original paper)\n",
        "        # B: Zero initialization (so ΔW = B@A starts at zero)\n",
        "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.1)\n",
        "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
        "        \n",
        "        print(f\"Created LoRA layer: {out_features}×{in_features} → rank {rank}\")\n",
        "        print(f\"   Original params: {in_features * out_features:,}\")\n",
        "        print(f\"   LoRA params: {rank * (in_features + out_features):,}\")\n",
        "        print(f\"   Reduction: {(in_features * out_features) / (rank * (in_features + out_features)):.1f}x\")\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Original computation: x @ W_0^T + b_0\n",
        "        original_output = self.original_layer(x)\n",
        "        \n",
        "        # LoRA computation: x @ (B @ A)^T = x @ A^T @ B^T\n",
        "        # Note: we need to transpose because PyTorch Linear does x @ W^T\n",
        "        lora_output = x @ self.lora_A.T @ self.lora_B.T * self.scaling\n",
        "        \n",
        "        # Combine: W_new = W_0 + B @ A\n",
        "        return original_output + lora_output\n",
        "    \n",
        "    def get_lora_parameters(self):\n",
        "        \"\"\"Return only the LoRA parameters for optimization\"\"\"\n",
        "        return [self.lora_A, self.lora_B]\n",
        "\n",
        "# Test our LoRA layer\n",
        "print(\"Testing LoRA layer...\")\n",
        "test_linear = nn.Linear(768, 768)\n",
        "test_input = torch.randn(1, 10, 768)  # Batch size 1, sequence length 10\n",
        "\n",
        "# Create LoRA version\n",
        "lora_layer = LoRALayer(test_linear, rank=16)\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    original_output = test_linear(test_input)\n",
        "    lora_output = lora_layer(test_input)\n",
        "    \n",
        "print(f\"Forward pass successful!\")\n",
        "print(f\"Output shapes match: {original_output.shape == lora_output.shape}\")\n",
        "print(f\"Difference (should be ~0 initially): {(original_output - lora_output).abs().max().item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.3 Applying LoRA to DistilBERT\n",
        "\n",
        "Apply our LoRA layers to the attention matrices in DistilBERT:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_lora_to_distilbert(model, rank=16):\n",
        "    \"\"\"Apply LoRA to all attention layers in DistilBERT\"\"\"\n",
        "    \n",
        "    print(f\"Applying LoRA (rank={rank}) to DistilBERT attention layers...\")\n",
        "    \n",
        "    lora_params = []\n",
        "    original_params = 0\n",
        "    lora_param_count = 0\n",
        "    \n",
        "    # Apply LoRA to each transformer layer's attention\n",
        "    for layer_idx, transformer_layer in enumerate(model.transformer.layer):\n",
        "        attention = transformer_layer.attention\n",
        "        \n",
        "        print(f\"\\nLayer {layer_idx}:\")\n",
        "        \n",
        "        # Apply to Query, Key, Value, and Output projections\n",
        "        for name, linear_layer in [\n",
        "            ('q_lin', attention.q_lin),\n",
        "            ('k_lin', attention.k_lin), \n",
        "            ('v_lin', attention.v_lin),\n",
        "            ('out_lin', attention.out_lin)\n",
        "        ]:\n",
        "            # Count original parameters\n",
        "            orig_params = sum(p.numel() for p in linear_layer.parameters())\n",
        "            original_params += orig_params\n",
        "            \n",
        "            # Replace with LoRA layer\n",
        "            lora_layer = LoRALayer(linear_layer, rank=rank)\n",
        "            setattr(attention, name, lora_layer)\n",
        "            \n",
        "            # Collect LoRA parameters\n",
        "            lora_params.extend(lora_layer.get_lora_parameters())\n",
        "            lora_param_count += len(lora_layer.get_lora_parameters()[0]) + len(lora_layer.get_lora_parameters()[1])\n",
        "            \n",
        "            print(f\"  {name}: LoRA applied\")\n",
        "    \n",
        "    print(f\"\\nLoRA Application Summary:\")\n",
        "    print(f\"Original attention parameters: {original_params:,}\")\n",
        "    print(f\"LoRA parameters added: {lora_param_count:,}\")\n",
        "    print(f\"Reduction factor: {original_params / lora_param_count:.1f}x\")\n",
        "    \n",
        "    return lora_params\n",
        "\n",
        "# Create a fresh DistilBERT model for LoRA\n",
        "print(\"Loading fresh DistilBERT for LoRA...\")\n",
        "lora_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased', \n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "lora_parameters = apply_lora_to_distilbert(lora_model, rank=16)\n",
        "\n",
        "print(f\"\\nReady for LoRA training with {len(lora_parameters):,} trainable parameters!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.4 LoRA Training and Results\n",
        "\n",
        "Now let's train our LoRA model and compare the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for LoRA training\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# LoRA Training setup\n",
        "print(\"Setting up LoRA training...\")\n",
        "\n",
        "# Only optimize LoRA parameters + classifier head\n",
        "optimizer = torch.optim.AdamW([\n",
        "    *lora_parameters,  # LoRA parameters\n",
        "    *lora_model.classifier.parameters()  # Classification head\n",
        "], lr=5e-4, weight_decay=0.01)\n",
        "\n",
        "# Simple training loop (faster than Trainer for this demo)\n",
        "def train_lora_model(model, train_data, test_data, epochs=2):\n",
        "    model.train()\n",
        "    \n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    \n",
        "    print(f\"Training LoRA for {epochs} epochs...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        # Training\n",
        "        for i in tqdm(range(0, len(train_data), 16), desc=f\"Epoch {epoch+1}\"):\n",
        "            batch = train_data[i:i+16]\n",
        "            \n",
        "            # Prepare batch\n",
        "            texts = [item['text'] for item in batch]\n",
        "            labels = torch.tensor([item['label'] for item in batch])\n",
        "            \n",
        "            inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, \n",
        "                             max_length=128, padding=True)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        train_losses.append(avg_loss)\n",
        "        \n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i in range(0, min(500, len(test_data)), 16):  # Quick eval\n",
        "                batch = test_data[i:i+16]\n",
        "                texts = [item['text'] for item in batch]\n",
        "                labels = [item['label'] for item in batch]\n",
        "                \n",
        "                inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True,\n",
        "                                 max_length=128, padding=True)\n",
        "                \n",
        "                outputs = model(**inputs)\n",
        "                predictions = outputs.logits.argmax(-1)\n",
        "                \n",
        "                correct += (predictions == torch.tensor(labels)).sum().item()\n",
        "                total += len(labels)\n",
        "        \n",
        "        accuracy = correct / total\n",
        "        test_accuracies.append(accuracy)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.3f}\")\n",
        "        model.train()\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    return train_losses, test_accuracies, training_time\n",
        "\n",
        "# Train LoRA model\n",
        "losses, accuracies, lora_time = train_lora_model(\n",
        "    lora_model, tokenized_train, tokenized_test, epochs=2\n",
        ")\n",
        "\n",
        "# Final evaluation\n",
        "lora_model.eval()\n",
        "final_accuracy = accuracies[-1] if accuracies else 0\n",
        "\n",
        "print(f\"\\nLoRA Training Complete!\")\n",
        "print(f\"Training time: {lora_time:.1f} seconds\")\n",
        "print(f\"Final accuracy: {final_accuracy:.3f}\")\n",
        "\n",
        "# Compare with full fine-tuning\n",
        "print(f\"\\nCOMPARISON SUMMARY:\")\n",
        "print(f\"{'Method':<20} {'Accuracy':<10} {'Time (s)':<10} {'Parameters':<15}\")\n",
        "print(f\"{'-'*55}\")\n",
        "print(f\"{'Baseline':<20} {baseline_acc:<10.3f} {'N/A':<10} {'0':<15}\")\n",
        "print(f\"{'LoRA':<20} {final_accuracy:<10.3f} {lora_time:<10.1f} {len(lora_parameters)*2:,}{'':>5}\")\n",
        "print(f\"{'Full Fine-tuning':<20} {full_finetuning_accuracy:<10.3f} {full_finetuning_time:<10.1f} {'66,955,010':<15}\")\n",
        "\n",
        "print(f\"\\nKey Takeaways:\")\n",
        "print(f\"• LoRA achieved {final_accuracy:.1%} accuracy with {(len(lora_parameters)*2/66955010)*100:.2f}% of parameters\")\n",
        "print(f\"• LoRA was {full_finetuning_time/lora_time:.1f}x faster to train\")\n",
        "print(f\"• LoRA performance: {final_accuracy/full_finetuning_accuracy*100:.1f}% of full fine-tuning accuracy\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses, 'b-', label='LoRA Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(accuracies, 'g-', label='LoRA Accuracy')\n",
        "plt.axhline(y=full_finetuning_accuracy, color='r', linestyle='--', label='Full Fine-tuning')\n",
        "plt.axhline(y=baseline_acc, color='gray', linestyle=':', label='Baseline')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCongratulations! You've successfully implemented and trained LoRA from scratch!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "**You've successfully implemented LoRA from scratch!** Here's what we accomplished:\n",
        "\n",
        "### What We Built\n",
        "1. **Mathematical Foundation**: Understood the low-rank decomposition W = W₀ + B@A\n",
        "2. **Custom LoRA Layer**: Built our own PyTorch implementation\n",
        "3. **Real Application**: Applied LoRA to DistilBERT for sentiment analysis\n",
        "4. **Performance Comparison**: Compared LoRA vs full fine-tuning\n",
        "\n",
        "### Key Results\n",
        "- **Parameter Efficiency**: ~24x fewer parameters than full fine-tuning\n",
        "- **Speed**: Significantly faster training (minutes vs hours)\n",
        "- **Performance**: Competitive accuracy with much lower resource requirements\n",
        "- **Memory**: Minimal additional memory footprint\n",
        "\n",
        "### When to Use LoRA\n",
        "**Good for:**\n",
        "- Large models where full fine-tuning is expensive\n",
        "- Multiple task adaptation (can store different LoRA weights)\n",
        "- Quick experimentation and prototyping\n",
        "- Resource-constrained environments\n",
        "\n",
        "**Consider alternatives when:**\n",
        "- You have unlimited compute and the model is small\n",
        "- You need the absolute best performance possible\n",
        "- The task requires very different representations than the pre-trained model\n",
        "\n",
        "### Next Steps\n",
        "- Try different rank values (r=4, 8, 32, 64)\n",
        "- Apply LoRA to other model components (feedforward layers)\n",
        "- Experiment with other datasets and tasks\n",
        "- Explore recent variants like AdaLoRA, QLoRA\n",
        "\n",
        "**Happy fine-tuning!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
