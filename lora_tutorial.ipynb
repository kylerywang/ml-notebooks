{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9GxiRDNa6RH"
      },
      "source": [
        "# LoRA\n",
        "\n",
        "##Transfer Learning\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805) and [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) popularized the approach of pre-training on large amounts of internet data, then fine-tuning on smaller datasets to perform specialized tasks. This approach of first teaching a model a broad skill (e.g. language modeling), and then re-using that model to learn a more specific task (e.g. sentiment analysis) is called transfer learning.\n",
        "\n",
        "Initially, transfer learning required creating an entirely new model for each new task during fine-tuning. This approach becomes computationally expensive and memory-intensive for larger models.\n",
        "\n",
        "### Adapters\n",
        "\n",
        "[Adapters](https://arxiv.org/abs/1902.00751) introduced a more efficient approach by inserting small bottleneck layers between existing transformer layers. These adapter modules add only a few trainable parameters per task while keeping the original pre-trained weights completely frozen. This way, you only need to train a small number of adapter parameters\n",
        "\n",
        "This reduced the computational load but came with limitations:\n",
        "1. **Inference Latency**: Adapters add extra layers, increasing computational overhead\n",
        "2. **Worse performance**: These methods sometimes underperform full fine-tuning\n",
        "\n",
        "Ideally, we'd like to maintain the same model architecture, as to avoid the latency problem. We'd also like to avoid updating the entire model, as to avoid the high computational cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBR0RPBMbEI6"
      },
      "source": [
        "## Enter LoRA\n",
        "\n",
        "**LoRA (Low-Rank Adaptation)** ([Hu et al., 2021](https://arxiv.org/abs/2106.09685)) provides a solution to this problem.\n",
        "\n",
        "When we fine-tune a model, we freeze the original model and apply weight updates $\\Delta W$. It turns out that $\\Delta W$ is much lower rank than the original weight matrix $W$ - meaning you can express the change without updating every single weight. This is the insight behind LoRA.\n",
        "\n",
        "Instead of updating the full weight matrix:\n",
        "$$W_{new} = W_0 + \\Delta W$$\n",
        "\n",
        "LoRA decomposes the update into two low-rank matrices:\n",
        "$$W_{new} = W_0 + \\Delta W = W_0 + BA$$\n",
        "\n",
        "Where:\n",
        "- $W_0 \\in \\mathbb{R}^{d \\times k}$ is the original frozen weight matrix\n",
        "- $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ are trainable low-rank matrices. These are initialized to zero.\n",
        "- $r \\ll \\min(d, k)$ is the rank (typically 1-64)\n",
        "\n",
        "### Parameter Reduction\n",
        "\n",
        "For a weight matrix of size $d \\times k$:\n",
        "- **Full fine-tuning**: $d \\times k$ parameters\n",
        "- **LoRA**: $r \\times (d + k)$ parameters\n",
        "\n",
        "**Reduction factor**: $\\frac{d \\times k}{r \\times (d + k)}$\n",
        "\n",
        "For large matrices, this can be **100x-1000x fewer parameters**!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KyYNgjJbBpx"
      },
      "source": [
        "## Build: Sentiment Analysis with LoRA\n",
        "\n",
        "We'll implement LoRA for sentiment analysis using the IMDB movie reviews dataset. We'll adapt a pre-trained DistilBERT model (66M params).\n",
        "\n",
        "1. **Baseline**: Test DistilBERT's zero-shot performance on IMDB\n",
        "2. **Resource Analysis**: Examine DistilBERT's architecture and parameter count\n",
        "3. **Implement LoRA**: Build LoRA from scratch and see the math in action\n",
        "4. **Training & Comparison**: Compare LoRA vs full fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cgoKljcbbj6"
      },
      "source": [
        "### 1. Setup and Data Loading\n",
        "\n",
        "First, let's install the required packages and load our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XoQM9q7GayEF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.46.1)\n",
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.1.0)\n",
            "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.1)\n",
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.7.0)\n",
            "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.9.2)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.26.3)\n",
            "Requirement already satisfied: accelerate in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.8.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.2.0)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: psutil in /Users/kylerwang/Library/Python/3.12/lib/python/site-packages (from accelerate) (6.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets torch scikit-learn matplotlib numpy accelerate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    DistilBertModel,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FU3DJJoUayEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading IMDB dataset...\n",
            "loaded 5000 training samples and 1000 test samples\n",
            "\n",
            "Sample reviews:\n",
            "\n",
            "Positive: There is no relation at all between Fortier and Profiler but the fact that both are police series ab...\n",
            "\n",
            "Positive: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. T...\n",
            "\n",
            "Negative: George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly...\n",
            "\n",
            "Positive: In the process of trying to establish the audiences' empathy with Jake Roedel (Tobey Maguire) the fi...\n",
            "\n",
            "Negative: Yeh, I know -- you're quivering with excitement. Well, *The Secret Lives of Dentists* will not upset...\n"
          ]
        }
      ],
      "source": [
        "# Load IMDB dataset\n",
        "print(\"Loading IMDB dataset...\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# using subset for faster experimentation (5k samples)\n",
        "SUBSET_SIZE = 5000\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(SUBSET_SIZE))\n",
        "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))  # 1k for testing\n",
        "\n",
        "print(f\"loaded {len(train_dataset)} training samples and {len(test_dataset)} test samples\")\n",
        "\n",
        "# look at a few examples\n",
        "print(\"\\nSample reviews:\")\n",
        "for i in range(5):\n",
        "    review = train_dataset[i]\n",
        "    label = \"Positive\" if review[\"label\"] == 1 else \"Negative\"\n",
        "    text_preview = review[\"text\"][:100] + \"...\" if len(review[\"text\"]) > 100 else review[\"text\"]\n",
        "    print(f\"\\n{label}: {text_preview}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFue-RQZbid0"
      },
      "source": [
        "### 2. Baseline: Pre-trained DistilBERT Performance\n",
        "\n",
        "Let's see how DistilBERT performs on sentiment analysis without any task-specific training. We'll use DistilBERT with a randomly initialized classification head to show that the pre-trained model alone cannot perform sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_pPlE7dyayEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading DistilBERT...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BASELINE EVALUATION ===\n",
            "Evaluating model on 200 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:06<00:00, 29.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline accuracy: 0.520\n",
            "(This is essentially random - the classification head is randomly initialized!)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# load pre-trained DistilBERT model and tokenizer\n",
        "print(\"Loading DistilBERT...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# load DistilBERT with classification head (untrained)\n",
        "baseline_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased', \n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "def evaluate_model(model, test_data, sample_size=1000):\n",
        "    \"\"\"\n",
        "    Standard evaluation function for any DistilBERT model\n",
        "    This function will be reused across baseline, fine-tuning, and LoRA\n",
        "    \"\"\"\n",
        "    # Move model to CPU for evaluation to avoid device issues\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(f\"Evaluating model on {sample_size} samples...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(min(sample_size, len(test_data)))):\n",
        "            text = test_data[i][\"text\"]\n",
        "            true_label = test_data[i][\"label\"]\n",
        "            \n",
        "            # Tokenize and get model prediction (inputs will be on CPU)\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
        "                             max_length=128, padding=True)\n",
        "            \n",
        "            outputs = model(**inputs)\n",
        "            predicted_label = outputs.logits.argmax(-1).item()\n",
        "\n",
        "            if predicted_label == true_label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Test baseline performance (untrained model)\n",
        "print(\"=== BASELINE EVALUATION ===\")\n",
        "baseline_acc = evaluate_model(baseline_model, test_dataset, sample_size=200)\n",
        "print(f\"Baseline accuracy: {baseline_acc:.3f}\")\n",
        "print(\"(This is essentially random - the classification head is randomly initialized!)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Maz-0fD1blDb"
      },
      "source": [
        "### 3. Resource Analysis: Understanding DistilBERT\n",
        "\n",
        "Before we implement LoRA, let's examine DistilBERT's architecture and see exactly where our parameters are and what we'd be updating.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JHUkFsuGayEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DistilBERT: 66,955,010 parameters (~255 MB)\n",
            "hidden_size: 768\n",
            "num_layers: 6\n",
            "Attention matrices: 14,155,776 parameters (21% of model)\n"
          ]
        }
      ],
      "source": [
        "# DistilBERT Parameter Overview\n",
        "total_params = sum(p.numel() for p in baseline_model.parameters())\n",
        "print(f\"DistilBERT: {total_params:,} parameters (~255 MB)\")\n",
        "print(f\"hidden_size: {baseline_model.config.dim}\")  # 768 for DistilBERT\n",
        "print(f\"num_layers: {len(baseline_model.distilbert.transformer.layer)}\")  # 6 layers\n",
        "\n",
        "\n",
        "# Focus on attention matrices (where LoRA works)\n",
        "attn_layer = baseline_model.distilbert.transformer.layer[0].attention\n",
        "attn_matrix_size = attn_layer.q_lin.weight.numel()\n",
        "total_attn_params = attn_matrix_size * 4 * 6  # 4 matrices × 6 layers\n",
        "print(f\"Attention matrices: {total_attn_params:,} parameters (21% of model)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKdOJiX2brk6"
      },
      "source": [
        "### 4. Optional: Full Fine-Tuning (⚠️ Time-Intensive)\n",
        "\n",
        "This section contains pre-written code for full fine-tuning comparison. This will provide an additional baseline to compare against for LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ckYskD4iayEH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting full fine-tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 50/626 [00:12<02:21,  4.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6759, 'grad_norm': 2.270479440689087, 'learning_rate': 2.5e-05, 'epoch': 0.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 100/626 [00:24<02:05,  4.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4809, 'grad_norm': 10.104291915893555, 'learning_rate': 5e-05, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 150/626 [00:36<02:05,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4232, 'grad_norm': 8.700576782226562, 'learning_rate': 4.524714828897338e-05, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 200/626 [00:49<01:49,  3.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4425, 'grad_norm': 6.786905765533447, 'learning_rate': 4.0494296577946774e-05, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            " 32%|███▏      | 200/626 [00:53<01:49,  3.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3943943679332733, 'eval_accuracy': 0.825, 'eval_runtime': 4.1498, 'eval_samples_per_second': 240.977, 'eval_steps_per_second': 15.182, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 250/626 [01:05<01:31,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4255, 'grad_norm': 9.531633377075195, 'learning_rate': 3.574144486692015e-05, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 300/626 [01:17<01:22,  3.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4107, 'grad_norm': 4.953176021575928, 'learning_rate': 3.098859315589354e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 350/626 [01:30<01:05,  4.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2896, 'grad_norm': 6.062676906585693, 'learning_rate': 2.6235741444866924e-05, 'epoch': 1.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 400/626 [01:42<00:55,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2187, 'grad_norm': 4.107425689697266, 'learning_rate': 2.1482889733840306e-05, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            " 64%|██████▍   | 400/626 [01:46<00:55,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4450661242008209, 'eval_accuracy': 0.827, 'eval_runtime': 3.8646, 'eval_samples_per_second': 258.759, 'eval_steps_per_second': 16.302, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 450/626 [01:58<00:43,  4.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.243, 'grad_norm': 0.21374252438545227, 'learning_rate': 1.673003802281369e-05, 'epoch': 1.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|███████▉  | 500/626 [02:10<00:29,  4.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2438, 'grad_norm': 1.6012609004974365, 'learning_rate': 1.1977186311787073e-05, 'epoch': 1.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 550/626 [02:22<00:18,  4.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2228, 'grad_norm': 6.345047473907471, 'learning_rate': 7.224334600760456e-06, 'epoch': 1.76}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 600/626 [02:34<00:06,  4.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2133, 'grad_norm': 9.516763687133789, 'learning_rate': 2.4714828897338406e-06, 'epoch': 1.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            " 96%|█████████▌| 600/626 [02:38<00:06,  4.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.42041751742362976, 'eval_accuracy': 0.842, 'eval_runtime': 3.798, 'eval_samples_per_second': 263.293, 'eval_steps_per_second': 16.587, 'epoch': 1.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 626/626 [02:44<00:00,  3.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 164.6927, 'train_samples_per_second': 60.719, 'train_steps_per_second': 3.801, 'train_loss': 0.3513666859830911, 'epoch': 2.0}\n",
            "Full fine-tuning completed in 165.3 seconds\n",
            "\n",
            "=== FULL FINE-TUNING EVALUATION ===\n",
            "Evaluating model on 1000 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:30<00:00, 32.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full fine-tuning accuracy: 0.848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Full Fine-tuning Implementation\n",
        "print(\"Starting full fine-tuning...\")\n",
        "\n",
        "# Use a copy of the baseline model for consistency\n",
        "import copy\n",
        "classification_model = copy.deepcopy(baseline_model)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Training arguments - optimized for speed\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,  # Reduced for time\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"no\",  # Don't save to reduce overhead\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=classification_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=lambda eval_pred: {\n",
        "        'accuracy': accuracy_score(eval_pred.label_ids, eval_pred.predictions.argmax(-1))\n",
        "    }\n",
        ")\n",
        "\n",
        "# Train\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "full_finetuning_time = time.time() - start_time\n",
        "\n",
        "print(f\"Full fine-tuning completed in {full_finetuning_time:.1f} seconds\")\n",
        "\n",
        "# Evaluate using the same function as baseline (for fair comparison)\n",
        "print(\"\\n=== FULL FINE-TUNING EVALUATION ===\")\n",
        "full_finetuning_accuracy = evaluate_model(classification_model, test_dataset, sample_size=1000)\n",
        "print(f\"Full fine-tuning accuracy: {full_finetuning_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub3qabMgayEH",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 5. LoRA Implementation: Step by Step\n",
        "\n",
        "Let's implement LoRA from scratch and see the mathematical decomposition in action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx4OWU_EayEH",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.1 The Math: Low-Rank Decomposition\n",
        "\n",
        "Recall our goal: instead of updating the full weight matrix W, we want to learn a low-rank update:\n",
        "\n",
        "**W_new = W_0 + ΔW = W_0 + B @ A**\n",
        "\n",
        "Where:\n",
        "- W_0: Original frozen weights (768 × 768 for DistilBERT attention)\n",
        "- B: Trainable matrix (768 × r)\n",
        "- A: Trainable matrix (r × 768)\n",
        "- r: Rank (we'll use r=16)\n",
        "\n",
        "Parameter savings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uKXgJBxjayEH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA Decomposition:\n",
            "W_new = W₀ + B@A\n",
            "Where B: 768×16, A: 16×768\n",
            "\n",
            "Parameter comparison per matrix:\n",
            "Full update: 589,824\n",
            "LoRA update: 24,576\n",
            "Reduction: 24x\n",
            "\n",
            "DistilBERT total: 589,824 LoRA parameters added\n"
          ]
        }
      ],
      "source": [
        "# LoRA parameter calculation\n",
        "d, k = 768, 768  # DistilBERT attention matrix dimensions\n",
        "r = 16          # LoRA rank\n",
        "\n",
        "print(\"LoRA Decomposition:\")\n",
        "print(f\"W_new = W₀ + B@A\")\n",
        "print(f\"Where B: {d}×{r}, A: {r}×{k}\")\n",
        "print()\n",
        "\n",
        "# Parameter counts\n",
        "full_params = d * k\n",
        "lora_params = r * (d + k)\n",
        "\n",
        "print(\"Parameter comparison per matrix:\")\n",
        "print(f\"Full update: {full_params:,}\")\n",
        "print(f\"LoRA update: {lora_params:,}\")\n",
        "print(f\"Reduction: {full_params // lora_params}x\")\n",
        "print()\n",
        "\n",
        "# Total for DistilBERT\n",
        "num_matrices = 4 * 6  # Q,K,V,O × 6 layers\n",
        "total_lora = lora_params * num_matrices\n",
        "print(f\"DistilBERT total: {total_lora:,} LoRA parameters added\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "VfkSomWXayEI",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.2 LoRA Layer Implementation\n",
        "\n",
        "Build our LoRA layer from scratch:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NEkArprYayEI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing LoRA layer...\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "Forward pass successful!\n",
            "Output shapes match: True\n",
            "Difference (should be ~0 initially): 0.000000\n"
          ]
        }
      ],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    LoRA (Low-Rank Adaptation) layer\n",
        "\n",
        "    Implements: output = input @ (W_0 + B @ A)\n",
        "    Where W_0 is frozen, and B, A are trainable low-rank matrices\n",
        "    \"\"\"\n",
        "    def __init__(self, original_layer, rank=16, alpha=16):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store the original layer (frozen)\n",
        "        self.original_layer = original_layer\n",
        "        self.original_layer.requires_grad_(False)  # Freeze original weights\n",
        "\n",
        "        # Get dimensions\n",
        "        if hasattr(original_layer, 'in_features'):  # Linear layer\n",
        "            in_features = original_layer.in_features\n",
        "            out_features = original_layer.out_features\n",
        "        else:\n",
        "            raise ValueError(\"LoRA currently supports nn.Linear layers\")\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank  # Scaling factor\n",
        "\n",
        "        # A: normal initialization (like original paper)\n",
        "        # B: zero initialization (so ΔW = B@A starts at zero)\n",
        "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.1)\n",
        "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
        "\n",
        "        print(f\"Created LoRA layer: {out_features}×{in_features} → rank {rank}\")\n",
        "        print(f\"   Original params: {in_features * out_features:,}\")\n",
        "        print(f\"   LoRA params: {rank * (in_features + out_features):,}\")\n",
        "        print(f\"   Reduction: {(in_features * out_features) / (rank * (in_features + out_features)):.1f}x\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Original computation: x @ W_0^T + b_0\n",
        "        original_output = self.original_layer(x)\n",
        "\n",
        "        # LoRA computation: x @ (B @ A)^T = x @ A^T @ B^T\n",
        "        # Note: we need to transpose because PyTorch Linear does x @ W^T\n",
        "        lora_output = x @ self.lora_A.T @ self.lora_B.T * self.scaling\n",
        "\n",
        "        # Combine: W_new = W_0 + B @ A\n",
        "        return original_output + lora_output\n",
        "\n",
        "    def get_lora_parameters(self):\n",
        "        \"\"\"Return only the LoRA parameters for optimization\"\"\"\n",
        "        return [self.lora_A, self.lora_B]\n",
        "\n",
        "# Test our LoRA layer\n",
        "print(\"Testing LoRA layer...\")\n",
        "test_linear = nn.Linear(768, 768)\n",
        "test_input = torch.randn(1, 10, 768)  # Batch size 1, sequence length 10\n",
        "\n",
        "# Create LoRA version\n",
        "lora_layer = LoRALayer(test_linear, rank=16)\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    original_output = test_linear(test_input)\n",
        "    lora_output = lora_layer(test_input)\n",
        "\n",
        "print(f\"Forward pass successful!\")\n",
        "print(f\"Output shapes match: {original_output.shape == lora_output.shape}\")\n",
        "print(f\"Difference (should be ~0 initially): {(original_output - lora_output).abs().max().item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "BYs5bIlhayEI",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.3 Applying LoRA to DistilBERT\n",
        "\n",
        "Apply our LoRA layers to the attention matrices in DistilBERT:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPjho8c5ayEI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating LoRA model from baseline...\n",
            "Applying LoRA (rank=16) to DistilBERT attention layers...\n",
            "\n",
            "Layer 0:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 1:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 2:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 3:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 4:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "Layer 5:\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  q_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  k_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  v_lin: LoRA applied\n",
            "Created LoRA layer: 768×768 → rank 16\n",
            "   Original params: 589,824\n",
            "   LoRA params: 24,576\n",
            "   Reduction: 24.0x\n",
            "  out_lin: LoRA applied\n",
            "\n",
            "LoRA Application Summary:\n",
            "Original attention parameters: 14,174,208\n",
            "LoRA parameters added: 18,816\n",
            "Reduction factor: 753.3x\n",
            "\n",
            "Ready for LoRA training with 48 trainable parameters!\n"
          ]
        }
      ],
      "source": [
        "def apply_lora_to_distilbert(model, rank=16):\n",
        "    \"\"\"Apply LoRA to all attention layers in DistilBERT\"\"\"\n",
        "\n",
        "    print(f\"Applying LoRA (rank={rank}) to DistilBERT attention layers...\")\n",
        "\n",
        "    lora_params = []\n",
        "    original_params = 0\n",
        "    lora_param_count = 0\n",
        "\n",
        "    # Apply LoRA to each transformer layer's attention\n",
        "    for layer_idx, transformer_layer in enumerate(model.distilbert.transformer.layer):\n",
        "        attention = transformer_layer.attention\n",
        "\n",
        "        print(f\"\\nLayer {layer_idx}:\")\n",
        "\n",
        "        # Apply to Query, Key, Value, and Output projections\n",
        "        for name, linear_layer in [\n",
        "            ('q_lin', attention.q_lin),\n",
        "            ('k_lin', attention.k_lin),\n",
        "            ('v_lin', attention.v_lin),\n",
        "            ('out_lin', attention.out_lin)\n",
        "        ]:\n",
        "            # Count original parameters\n",
        "            orig_params = sum(p.numel() for p in linear_layer.parameters())\n",
        "            original_params += orig_params\n",
        "\n",
        "            # Replace with LoRA layer\n",
        "            lora_layer = LoRALayer(linear_layer, rank=rank)\n",
        "            setattr(attention, name, lora_layer)\n",
        "\n",
        "            # Collect LoRA parameters\n",
        "            lora_params.extend(lora_layer.get_lora_parameters())\n",
        "            lora_param_count += len(lora_layer.get_lora_parameters()[0]) + len(lora_layer.get_lora_parameters()[1])\n",
        "\n",
        "            print(f\"  {name}: LoRA applied\")\n",
        "\n",
        "    print(f\"\\nLoRA Application Summary:\")\n",
        "    print(f\"Original attention parameters: {original_params:,}\")\n",
        "    print(f\"LoRA parameters added: {lora_param_count:,}\")\n",
        "    print(f\"Reduction factor: {original_params / lora_param_count:.1f}x\")\n",
        "\n",
        "    return lora_params\n",
        "\n",
        "# Use a copy of the baseline model for consistency  \n",
        "print(\"Creating LoRA model from baseline...\")\n",
        "lora_model = copy.deepcopy(baseline_model)\n",
        "\n",
        "# Apply LoRA\n",
        "lora_parameters = apply_lora_to_distilbert(lora_model, rank=16)\n",
        "\n",
        "print(f\"\\nReady for LoRA training with {len(lora_parameters):,} trainable parameters!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "pVVOnYz2ayEI",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 5.4 LoRA Training and Results\n",
        "\n",
        "Now let's train our LoRA model and compare the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s0jauWayEI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up LoRA training...\n",
            "Training LoRA for 2 epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 313/313 [04:36<00:00,  1.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 0.4421, Accuracy = 0.834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 313/313 [04:20<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss = 0.3050, Accuracy = 0.828\n",
            "\n",
            "LoRA Training Complete!\n",
            "Training time: 558.3 seconds\n",
            "Final accuracy: 0.828\n",
            "\n",
            "COMPARISON SUMMARY:\n",
            "Method               Accuracy   Time (s)   Parameters     \n",
            "-------------------------------------------------------\n",
            "Baseline             0.520      N/A        0              \n",
            "LoRA                 0.828      558.3      96     \n",
            "Full Fine-tuning     0.848      165.3      66,955,010     \n",
            "\n",
            "Key Takeaways:\n",
            "• LoRA achieved 82.8% accuracy with 0.00% of parameters\n",
            "• LoRA was 0.3x faster to train\n",
            "• LoRA performance: 97.7% of full fine-tuning accuracy\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Congratulations! You've successfully implemented and trained LoRA from scratch!\n"
          ]
        }
      ],
      "source": [
        "# LoRA Training setup\n",
        "print(\"Setting up LoRA training...\")\n",
        "\n",
        "# Only optimize LoRA parameters + classifier head\n",
        "optimizer = torch.optim.AdamW([\n",
        "    *lora_parameters,  # LoRA parameters\n",
        "    *lora_model.classifier.parameters()  # Classification head\n",
        "], lr=5e-4, weight_decay=0.01)\n",
        "\n",
        "# Simple training loop using pre-tokenized data (for fair comparison)\n",
        "def train_lora_model(model, train_data, test_data, epochs=2):\n",
        "    model.train()\n",
        "\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    print(f\"Training LoRA for {epochs} epochs...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Training\n",
        "        for i in tqdm(range(0, len(train_data), 16), desc=f\"Epoch {epoch+1}\"):\n",
        "            # Get batch from pre-tokenized dataset\n",
        "            batch_indices = list(range(i, min(i+16, len(train_data))))\n",
        "            \n",
        "            # Extract pre-tokenized inputs and labels\n",
        "            input_ids = torch.stack([torch.tensor(train_data[j]['input_ids']) for j in batch_indices])\n",
        "            attention_mask = torch.stack([torch.tensor(train_data[j]['attention_mask']) for j in batch_indices])\n",
        "            labels = torch.tensor([train_data[j]['label'] for j in batch_indices])\n",
        "            \n",
        "            inputs = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask\n",
        "            }\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, min(500, len(test_data)), 16):  # Quick eval\n",
        "                # Get batch from pre-tokenized dataset\n",
        "                batch_indices = list(range(i, min(i+16, len(test_data))))\n",
        "                \n",
        "                input_ids = torch.stack([torch.tensor(test_data[j]['input_ids']) for j in batch_indices])\n",
        "                attention_mask = torch.stack([torch.tensor(test_data[j]['attention_mask']) for j in batch_indices])\n",
        "                labels = torch.tensor([test_data[j]['label'] for j in batch_indices])\n",
        "                \n",
        "                inputs = {\n",
        "                    'input_ids': input_ids,\n",
        "                    'attention_mask': attention_mask\n",
        "                }\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                predictions = outputs.logits.argmax(-1)\n",
        "\n",
        "                correct += (predictions == labels).sum().item()\n",
        "                total += len(labels)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        test_accuracies.append(accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.3f}\")\n",
        "        model.train()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    return train_losses, test_accuracies, training_time\n",
        "\n",
        "# Train LoRA model using the SAME pre-tokenized data as full fine-tuning\n",
        "# This makes for a much fairer speed comparison!\n",
        "print(\"Using pre-tokenized datasets from full fine-tuning section...\")\n",
        "losses, accuracies, lora_time = train_lora_model(\n",
        "    lora_model, tokenized_train, tokenized_test, epochs=2\n",
        ")\n",
        "\n",
        "# Final evaluation\n",
        "lora_model.eval()\n",
        "final_accuracy = accuracies[-1] if accuracies else 0\n",
        "\n",
        "print(f\"\\nLoRA Training Complete!\")\n",
        "print(f\"Training time: {lora_time:.1f} seconds\")\n",
        "print(f\"Final accuracy: {final_accuracy:.3f}\")\n",
        "\n",
        "# Compare with full fine-tuning\n",
        "print(f\"\\nCOMPARISON SUMMARY:\")\n",
        "print(f\"{'Method':<20} {'Accuracy':<10} {'Time (s)':<10} {'Parameters':<15}\")\n",
        "print(f\"{'-'*55}\")\n",
        "print(f\"{'Baseline':<20} {baseline_acc:<10.3f} {'N/A':<10} {'0':<15}\")\n",
        "print(f\"{'LoRA':<20} {final_accuracy:<10.3f} {lora_time:<10.1f} {len(lora_parameters)*2:,}{'':>5}\")\n",
        "print(f\"{'Full Fine-tuning':<20} {full_finetuning_accuracy:<10.3f} {full_finetuning_time:<10.1f} {'66,955,010':<15}\")\n",
        "\n",
        "print(f\"\\nKey Takeaways:\")\n",
        "print(f\"• LoRA achieved {final_accuracy:.1%} accuracy with {(len(lora_parameters)*2/66955010)*100:.2f}% of parameters\")\n",
        "print(f\"• LoRA was {full_finetuning_time/lora_time:.1f}x faster to train\")\n",
        "print(f\"• LoRA performance: {final_accuracy/full_finetuning_accuracy*100:.1f}% of full fine-tuning accuracy\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses, 'b-', label='LoRA Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(accuracies, 'g-', label='LoRA Accuracy')\n",
        "plt.axhline(y=full_finetuning_accuracy, color='r', linestyle='--', label='Full Fine-tuning')\n",
        "plt.axhline(y=baseline_acc, color='gray', linestyle=':', label='Baseline')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCongratulations! You've successfully implemented and trained LoRA from scratch!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
